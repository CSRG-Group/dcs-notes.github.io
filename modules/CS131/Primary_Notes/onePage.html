<!DOCTYPE html>
<html lang=" en-US">

<head>

    
    <meta charset="UTF-8"><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- Include tocNAV javascript -->
    <script type="text/javascript" src="/assets/js/tocNav.js"></script>
    
    <!-- seo used to be here -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style"
        type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link id="mainCS" rel="stylesheet" href="/assets/css/style.css">
    <title>onePage 131 One Page Notes</title>
</head>

<body>
    <div id="mainGrid" class="container">
        <header style="padding:10px;" class="page-header notes-header" role="banner">
            
            
            <h1 class="project-name">131 One Page Notes</h1>
        </header>
        <div title="Table of Contents" class="buttonCol" onclick="toggleNav()">
            <div class="navArrow">
                <i></i>
            </div>
        </div>
        <div class="navBox">
            <div id="sidenav" class="sideNav closedNav">
                <h2 style="margin-left: 10px;">Table of Contents</h2><ul class="table-of-contents"><li><a href="#number-systems">Number Systems</a><ul><li><a href="#integers">Integers</a><ul><li><a href="#decimal-to-binary-conversion-algorithm">Decimal to Binary Conversion Algorithm</a><ul><li><a href="#problems">Problems</a></li></ul></li><li><a href="#generic-base-conversion-algorithm">Generic Base Conversion Algorithm</a></li><li><a href="#division-algorithm">Division Algorithm</a></li><li><a href="#euclidean-algorithm">Euclidean algorithm</a><ul><li><a href="#problems-1">Problems</a></li></ul></li><li><a href="#modular-arithmetic">Modular Arithmetic</a></li><li><a href="#twos-complement">Two’s complement</a></li></ul></li><li><a href="#reals">Reals</a><ul><li><a href="#properties-of-real-numbers">Properties of real numbers</a><ul><li><a href="#exercises">Exercises</a></li><li><a href="#problems-2">Problems</a></li></ul></li><li><a href="#intervals">Intervals</a></li><li><a href="#nth-roots">Nth roots</a></li><li><a href="#modulus-or-absolute-function">Modulus or absolute function</a></li><li><a href="#bounds">Bounds</a><ul><li><a href="#completeness-axiom">Completeness axiom</a></li></ul></li></ul></li><li><a href="#complex-numbers">Complex Numbers</a><ul><li><a href="#complex-conjugates">Complex conjugates</a></li><li><a href="#polar-coordinates">Polar coordinates</a></li><li><a href="#complex-modulus">Complex modulus</a></li><li><a href="#de-moivres-theorem">De Moivre's Theorem</a></li><li><a href="#fundamental-theorem-of-algebra">Fundamental Theorem of Algebra</a></li><li><a href="#other-notation">Other Notation</a></li></ul></li></ul></li><li><a href="#vectors">Vectors</a><ul><li><a href="#basics">Basics</a><ul><li><a href="#addition-and-scalar-multiplication">Addition and scalar multiplication</a></li><li><a href="#position-unit-and-zero-vectors-and-vector-length">Position, unit and zero vectors, and vector length</a></li><li><a href="#the-scalardot-product">The scalar/dot product</a></li></ul></li><li><a href="#linear-combinations-and-subspaces">Linear Combinations and Subspaces</a><ul><li><a href="#linear-combinations">Linear Combinations</a></li><li><a href="#spans">Spans</a></li><li><a href="#subspaces">Subspaces</a><ul><li><a href="#properties-of-subspaces">Properties of Subspaces</a></li></ul></li><li><a href="#exercise">Exercise</a></li></ul></li><li><a href="#linear-independence">Linear Independence</a></li><li><a href="#basis-and-dimension">Basis and Dimension</a><ul><li><a href="#basis">Basis</a></li><li><a href="#dimension">Dimension</a><ul><li><a href="#exercises">Exercises</a></li></ul></li><li><a href="#properties-of-bases">Properties of bases</a></li><li><a href="#change-of-basis">Change of basis</a></li></ul></li></ul></li><li><a href="#matrices">Matrices</a><ul><li><a href="#matrix-algebra">Matrix Algebra</a><ul><li><a href="#addition-and-scalar-multiplication">Addition and scalar multiplication</a><ul><li><a href="#properties-of-addition-and-scalar-multiplication">Properties of addition and scalar multiplication</a></li></ul></li><li><a href="#matrix-multiplication">Matrix multiplication</a><ul><li><a href="#properties-of-matrix-multiplication">Properties of matrix multiplication</a></li></ul></li><li><a href="#matrix-transposition">Matrix transposition</a><ul><li><a href="#properties-of-transposition">Properties of transposition</a></li></ul></li><li><a href="#types-of-matrices">Types of matrices</a></li><li><a href="#determinant-of-a-2x2-matrix">Determinant of a 2x2 matrix</a></li></ul></li><li><a href="#matrix-inverse-linear-equations">Matrix Inverse, Linear Equations</a><ul><li><a href="#elementary-row-operations">Elementary row operations</a></li><li><a href="#augmented-matrices">Augmented matrices</a></li></ul></li><li><a href="#matrix-inverse-determinants">Matrix Inverse, Determinants</a><ul><li><a href="#determinant-of-a-3x3-matrix-and-the-cofactor-matrix">Determinant of a 3x3 matrix and the cofactor matrix</a></li><li><a href="#elementary-row-operations-on-determinants">Elementary row operations on determinants</a></li><li><a href="#cramers-rule-to-invert-matrices">Cramer’s rule to invert matrices</a></li><li><a href="#linear-independence-via-determinants">Linear independence via determinants</a></li></ul></li><li><a href="#linear-transformations">Linear Transformations</a></li><li><a href="#linear-transformations-and-matrices">Linear Transformations and Matrices</a></li><li><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a><ul><li><a href="#diagonalisation-of-matrices">Diagonalisation of Matrices</a></li></ul></li></ul></li><li><a href="#sequences-and-series">Sequences and Series</a><ul><li><a href="#sequences">Sequences</a><ul><li><a href="#convergent-sequences">Convergent Sequences</a><ul><li><a href="#combination-rules-for-convergent-sequences">Combination Rules for Convergent Sequences</a></li><li><a href="#exercise">Exercise</a></li></ul></li><li><a href="#bounded-sequences">Bounded sequences</a></li><li><a href="#subsequences">Subsequences</a></li><li><a href="#basic-properties-of-convergent-sequences">Basic Properties of Convergent Sequences</a><ul><li><a href="#exercise-1">Exercise</a></li></ul></li><li><a href="#divergent-sequences">Divergent Sequences</a></li><li><a href="#basic-convergent-sequences">Basic convergent sequences</a></li></ul></li><li><a href="#series">Series</a><ul><li><a href="#basic-properties-of-convergent-series">Basic Properties of Convergent Series</a></li><li><a href="#the-comparison-test">The Comparison Test</a><ul><li><a href="#exercises">Exercises</a></li></ul></li><li><a href="#the-ratio-test">The Ratio Test</a></li><li><a href="#basic-convergent-series">Basic Convergent Series</a></li><li><a href="#basic-divergent-series">Basic Divergent Series</a></li><li><a href="#power-series">Power series</a><ul><li><a href="#basic-properties-of-power-series">Basic Properties of Power Series</a></li></ul></li><li><a href="#radius-of-convergence">Radius of convergence</a></li><li><a href="#addendum-partial-fractions">Addendum: Partial Fractions</a></li></ul></li><li><a href="#recurrences">Recurrences</a><ul><li><a href="#homogenous-recurrences">Homogenous recurrences</a></li><li><a href="#exercise-2">Exercise</a></li><li><a href="#non-homogenous-recurrences">Non-homogenous recurrences</a></li></ul></li><li><a href="#decimal-representation-of-reals">Decimal Representation of Reals</a></li></ul></li><li><a href="#calculus">Calculus</a><ul><li><a href="#limits-and-continuity">Limits and Continuity</a><ul><li><a href="#floor-and-ceiling-functions">Floor and Ceiling functions</a><ul><li><a href="#exercise">Exercise</a></li></ul></li><li><a href="#combination-rules-for-limits">Combination rules for limits</a></li><li><a href="#squeeze-rule-for-limits">Squeeze rule for limits</a></li><li><a href="#continuous-functions">Continuous functions</a><ul><li><a href="#combination-rules-for-continuous-functions">Combination rules for continuous functions</a></li><li><a href="#basic-continuous-functions">Basic continuous functions</a></li></ul></li><li><a href="#intermediate-value-theorem">Intermediate Value Theorem</a><ul><li><a href="#exercise-1">Exercise</a></li></ul></li><li><a href="#extreme-value-theorem">Extreme Value Theorem</a></li></ul></li><li><a href="#differentiation">Differentiation</a><ul><li><a href="#combination-rules-for-derivatives">Combination Rules for Derivatives</a></li><li><a href="#trig-derivatives">Trig derivatives</a></li><li><a href="#the-chain-rule">The Chain Rule</a></li><li><a href="#differentiation-of-functions-def-by-power-series">Differentiation of functions def' by power series</a><ul><li><a href="#exercise-2">Exercise</a></li></ul></li></ul></li><li><a href="#properties-of-differentiable-functions">Properties of Differentiable Functions</a><ul><li><a href="#turning-point-theorem">Turning Point Theorem</a></li><li><a href="#rolles-theorem">Rolle's Theorem</a></li><li><a href="#mean-value-theorem">Mean Value Theorem</a><ul><li><a href="#consequences-of-the-mvt">Consequences of the MVT</a></li></ul></li></ul></li><li><a href="#lhopitals-rule-and-implicit-differentiation">L'Hopital's Rule and Implicit Differentiation</a><ul><li><a href="#lhopitals-rule">L'Hopital's Rule</a></li><li><a href="#implicit-differentiation">Implicit differentiation</a><ul><li><a href="#exercise-3">Exercise</a></li></ul></li></ul></li><li><a href="#differentiating-inverse-functions">Differentiating Inverse Functions</a><ul><li><a href="#example">Example</a></li><li><a href="#trigonometric-inverse-derivatives">Trigonometric Inverse Derivatives</a></li></ul></li><li><a href="#integration">Integration</a><ul><li><a href="#definition-of-integration">Definition of integration</a></li><li><a href="#integration-by-substitution">Integration by Substitution</a></li><li><a href="#integration-by-parts">Integration by Parts</a></li><li><a href="#properties-of-definite-intervals">Properties of definite intervals</a></li><li><a href="#fundamental-theorems-of-calculus">Fundamental theorems of calculus</a><ul><li><a href="#first-fundamental-theorem-of-calculus">First Fundamental Theorem of Calculus</a></li><li><a href="#second-fundamental-theorem-of-calculus">Second Fundamental Theorem of Calculus</a></li></ul></li></ul></li><li><a href="#logs-and-exponentials">Logs and Exponentials</a><ul><li><a href="#properties-of-logarithms">Properties of Logarithms</a></li><li><a href="#properties-of-exponentials">Properties of Exponentials</a></li></ul></li><li><a href="#taylors-theorem">Taylor's Theorem</a><ul><li><a href="#taylors-theorem-1">Taylor's Theorem</a></li><li><a href="#taylor-series">Taylor series</a></li><li><a href="#nth-derivative-test-for-the-nature-of-stationary-points">Nth derivative test for the nature of stationary points</a></li><li><a href="#maclaurin-series">Maclaurin Series</a><ul><li><a href="#important-maclaurin-series">Important Maclaurin Series</a></li></ul></li></ul></li><li><a href="#first-order-odes">First Order ODEs</a><ul><li><a href="#separable-equations">Separable Equations</a></li><li><a href="#homogenous-equations">Homogenous Equations</a><ul><li><a href="#exercise-4">Exercise</a></li></ul></li><li><a href="#linear-equations">Linear Equations</a><ul><li><a href="#exercise-5">Exercise</a></li></ul></li></ul></li><li><a href="#second-order-odes">Second Order ODEs</a><ul><li><a href="#solving-homogenous-equations">Solving homogenous equations</a></li><li><a href="#finding-particular-solutions">Finding particular solutions</a></li></ul></li></ul></li></ul>
</div>
        </div>
        
        <div class="contents">
            <main id="content" class="main-content" role="main">
                <div class="partNav"><a href="../">🏡Module Home</a></div>
                <!-- Main Content of markdown or sub-layouts-->
                <!-- Layout for One Page Notes -->
<!-- 
    Works for all modules as long as they 
    - Are defined in the relevant module data file
-->





<!-- not the best and a little O-horrible  but the easiest way to do it --><h1 id="number-systems">Number Systems</h1>
       
            <h2 id="integers">Integers</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part1/note1.pdf">Link to the PDF.</a></p>

<p>Integers, denoted \(\mathbb{Z}\). We use a denary/decimal (base-10) system
with 10 digits, but there is also binary, which uses 2 digits.</p>

<p>Subscripts are used to denote base;
\(10011_{\textrm{two}} = 19_{\textrm{ten}}\) or \(10011_{2} = 19_{10}\).</p>

<h3 id="decimal-to-binary-conversion-algorithm">Decimal to Binary Conversion Algorithm</h3>

<p>Divide decimal number repeatedly
by 2 to get remainders \(r_0, r_1, r_2,...r_n\). The binary representation
is \(r_{n}r_{n-1}...r_{1}r_0\) (note the switched order).</p>

<h4 id="problems">Problems</h4>

<ol>
  <li>Convert \(244_{10}\) to binary.</li>
  <li>The <em>hexadecimal</em> system is base 16, with digits
0123456789ABCDEF. Convert \(21BAD_{16}\) to decimal.</li>
</ol>

<h3 id="generic-base-conversion-algorithm">Generic Base Conversion Algorithm</h3>

<p>Let us have an integer \(b\). To
convert a base 10 integer to a base \(b\) integer, divide repeatedly by
\(b\) to get remainders \(r_0, r_1,...,r_n\), thus the base \(b\)
representation would be \(r_{n},r_{n-1},...,r_{1},r_0\) (note the switched
order).</p>

<h3 id="division-algorithm">Division Algorithm</h3>

<p>If \(a, b \in \mathbb{Z}\) with \(b \neq 0\), then
there exist unique \(q, r \in \mathbb{Z}\) with</p>

\[\begin{align}  &amp; a = qb +
r,  &amp; 0 \leq r &lt; \|b\| \end{align}\]

<p>Where \(q\) is the <strong>quotient</strong> and
\(r\) the <strong>remainder</strong>.</p>

<p>If \(0 &lt; b &lt; a\) then an algorithm to compute \(q, r\) would be to
iteratively compute \(a - b, a - 2b, ..., a-nb, a - (n-1)b\) where
\(a - (n-1)b &lt; 0\) is the first strictly negative number. Then
\(q = n, r = a-nb\).</p>

<p>If \(a = qb\) (all integers) then \(b\) <em>divides</em> \(a\). The greatest common
denominator is denoted \(\gcd(a, b)\).</p>

\[\gcd(0, n) = n \; \forall n \in \mathbb{Z}^+\]

<h3 id="euclidean-algorithm">Euclidean algorithm</h3>

<p>Let \(r_1, r_0\) be integers s.t.
\(0 &lt; r_1 &lt; r_0\).</p>

<ol>
  <li>For each \(i\), define \(r_{i+1}\) as the remainder of
\(\frac{r_{i-1}}{r_i}\).<br />
The <strong>last</strong> non-zero remainder \(r_N = \gcd(r_1, r_0)\).</li>
  <li>\(r_{i-1} = q_i r_i + r_{i+1} \; (1 \leq 1 \leq N\) can be used to
write this last nonzero remainder, thus
\(\gcd(r_1, r_0) = xr_1 + yr_0 \textrm{ for some integers } x, y.\)</li>
</ol>

<p>A python implementation of this algorithm is as follows, and can be downloaded <a href="./gcd.py">here</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/usr/bin/python3
</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">log10</span><span class="p">,</span> <span class="n">ceil</span>

<span class="k">def</span> <span class="nf">printTableRow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">r</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"| {} | {} | {} |"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">).</span><span class="n">ljust</span><span class="p">(</span><span class="n">strPad</span><span class="p">),</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">b</span><span class="p">).</span><span class="n">ljust</span><span class="p">(</span><span class="n">strPad</span><span class="p">),</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">r</span><span class="p">).</span><span class="n">ljust</span><span class="p">(</span><span class="n">strPad</span><span class="p">)</span>
    <span class="p">))</span>

<span class="n">a</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="p">(</span><span class="s">"Enter the first number: "</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="p">(</span><span class="s">"Enter the second number: "</span><span class="p">))</span>
<span class="n">strPad</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">log10</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">log10</span><span class="p">(</span><span class="n">b</span><span class="p">)))</span>

<span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">b</span><span class="p">:</span>
    <span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="p">,</span><span class="n">a</span>
<span class="n">r</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="k">while</span> <span class="n">r</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">a</span> <span class="o">%</span> <span class="n">b</span>
    <span class="n">printTableRow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">r</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="p">,</span><span class="n">r</span>

<span class="k">print</span><span class="p">(</span><span class="s">"GCD = {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

</code></pre></div></div>

<h4 id="problems-1">Problems</h4>

<ol>
  <li>Find the greatest common divisor of 16579 and 30031, and
determine integers \(x\) and \(y\) such that
\(\gcd(16579, 30031) = x16579 + y30031\).</li>
</ol>

<h3 id="modular-arithmetic">Modular Arithmetic</h3>

<p>Two integers \(a, b\) are congruent modulo \(n\)
(another integer) if \(a-b = kn \; k \in \mathbb{Z}\), i.e. \(a = b + kn\).
Written \(a \equiv b \mod n\) or \(a \stackrel{\mod{}}{\equiv} b\).</p>

<p>Two congruencies with the same mod \(n\) can be added, subtracted,
multiplied just like normal equations.</p>

<h3 id="twos-complement">Two’s complement</h3>

<p>In a computer, for negative integers we use two's complement, see
<a href="../cs132/index.html#datarep">here</a>.</p>

<p>We are also working with a system modulo \(2^N\) where \(N\) is the number
of bits. Thus the 32 bit limit of \([2^{31}, 2^{31} - 1]\) which is
\(2^{32}\) integers.</p>

<h2 id="reals">Reals</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part1/note2.pdf">Link to the PDF.</a></p>

<p>Reals are denoted \(\mathbb{R}\), and have a subset \(\mathbb{Q}\) which are
numbers definable as \(\frac{m}{n}, \; m, n \in \mathbb{Z}, \; n \neq 0\).
We can always pick rationals such that \(n \geq 1\) and \(\gcd(m, n) = 1\).
Every nonzero rational has an inverse.</p>

<p>Reals which are the solutions of polynomial equations are called
<strong>algebraic</strong>, an example would be \(\sqrt{2}\) (being the solution of
\(x^2 = 2\)).</p>

<p>Those which are not are called <strong>transcendental</strong>, such as \(\pi, e\).</p>

<p>A real number can be thought of as a sequence of rational numbers, which
converges to said real. e.g. \(\pi\) is the limit of
\(3, 3.1, 3.14, 3.141, 3.1415...\)</p>

<h3 id="properties-of-real-numbers">Properties of real numbers</h3>

<p>All properties of real numbers come from <strong>13 axioms</strong>, of which 1-8 are
<strong>algebraic</strong> properties, and 9-12 are <strong>order properties</strong>.</p>

<p>For all \(x, y, z \in \mathbb{R}\):</p>

<ol>
  <li><strong>Commutativity</strong>: \(x + y = y + x, \; xy = yx\)</li>
  <li><strong>Associativity</strong>: \(x + (y + z) = (x + y) + z\) (same with
multiplication)</li>
  <li>Multiply <strong>distributes</strong> over add: \(x(y + z) = xy + xz\)</li>
  <li><strong>Additive Identity</strong>: \(\exists 0 \in \mathbb{R }: x + 0 = x\)</li>
  <li><strong>Multiplicative identity</strong>:
\(\exists 1 \in \mathbb{R} : x \cdot 1 = x\)</li>
  <li>Multiplicative and additive identites are <strong>distinct</strong>: \(1 \neq 0\)</li>
  <li>Every element has an <strong>additive inverse</strong>:
\(\exists (-x) \in \mathbb{R}: x + (-x) = 0\)</li>
  <li>Every \(\neq 0\) element has a <strong>mul. inverse</strong>:
\(x \neq 0 \implies \exists x^{-1} \in \mathbb{R} : x \cdot x^{-1} = 1\)</li>
  <li><strong>Transitivity</strong> of ordering: \(x &lt; y \land y &lt; z \implies x &lt; z\)</li>
  <li><strong>Trichotomy Law</strong>: only <strong>one</strong> of \(x &lt; y,\; x &gt; y,\; x = y\)</li>
  <li><strong>Order preserved</strong> under add: \(x &lt; y \implies x + z &lt; y + z\)</li>
  <li><strong>Order preserved</strong> under mul: \(0 &lt; z \land x &lt; y \implies xz &lt; yz\)</li>
  <li><strong>Completeness</strong>: <em>Every non-empty subset of \(\mathbb{R}\) that is
bounded above has a least upper bound</em></li>
</ol>

<h4 id="exercises">Exercises</h4>

<p><strong>1.</strong> Show that \(0 &lt; 1\).</p>

<p>We first need a lemma.</p>

<p><strong><em>Lemma 1.</em></strong> \(\forall x ,\; x^2 \geq 0\).</p>

<p><strong><em>Proof of Lemma 1.</em></strong> Consider the cases where \(x &lt; 0\) and \(x \geq 0\).</p>

<p>When \(x \geq 0\),</p>

\[\begin{align} x \geq 0  &amp; \implies x^2 \geq 0 \cdot
0 \textrm{ by ax. 12}\\  &amp; \implies \geq 0. \end{align}\]

<p>When \(x &lt; 0\), \(-x \geq 0\), so:</p>

\[\begin{align} x^2 = x \cdot x  &amp; =
(-x)(-x) \\  &amp; \geq 0 \cdot 0 \textrm{ by prev case}\\  &amp; \geq 0.
 &amp; \triangleright \end{align}\]

<p>Then, we can prove.
<strong><em>Proof.</em></strong></p>

\[\begin{align} 1^2  &amp; \geq 0 \textrm{ by lemma}\\
\implies 1  &amp; \geq 0\\ 1  &amp; \neq 0 \textrm{ by ax. 6 } \therefore 1 &gt; 0.
 &amp; \triangleright \end{align}\]

<p><strong>2.</strong> Show that \(a &gt; 0 \implies \frac{1}{a} &gt; 0\).</p>

<p><strong><em>Proof.</em></strong> Let \(a &gt; 0\)</p>

\[\begin{align} \textrm{If } \frac{1}{a}  &amp; = 0
\\ \implies 1  &amp; = a(\frac{1}{a}) = a \cdot 0 = 0 \\
 &amp; \textrm{which is a contradiction to ax. 6.} \end{align}\]

\[\begin{align} \textrm{If } \frac{1}{a}  &amp; &lt; 0 \\ \implies a
(\frac{1}{a})  &amp; &lt; a \cdot 0 \textrm{ by ax. 12} \\ \implies 1  &amp; = 0
\textrm{ which is a contradiction.} \end{align}\]

<p>By axiom 10 we get
that \(\frac{1}{a} &gt; 0. \quad \triangleright\)</p>

<h4 id="problems-2">Problems</h4>

<ol>
  <li>
    <p>Show if \(a, b &gt; 0\) then \(a &lt; b \Longleftrightarrow a^2 &lt; b^2\).
<em>Note: you have to prove both ways.</em></p>
  </li>
  <li>
    <p>Show if \(a &lt; b \land c &lt; 0 \implies ac &gt; bc\).</p>
  </li>
</ol>

<h3 id="intervals">Intervals</h3>

<p><strong>Intervals</strong> are ranges represented by brackets. \((a, b)\) ranges are
called <strong>open</strong>, \([a, b), \; (a, b]\) are called <strong>semi-open</strong> or
<strong>semi-closed</strong> and \([a, b]\) ranges are called <strong>closed</strong>. \(\infty\) is
not a real number and cannot appear in closed ranges.</p>

<h3 id="nth-roots">Nth roots</h3>

<p>Let \(n \in \mathbb{Z}^+\). For any
\(a \in \mathbb{R}_{\geq 0}, \; \exists! x \geq 0\) with \(x^n = a\). This
\(x\) is the <strong>\(n^{th}\) root</strong> of \(a\), \(a^{\frac{1}{n}}\). For any positive
real \(a, b\) and \(n \in \mathbb{Z}^+\) we have
\(a &lt; b \Longleftrightarrow a^{\frac{1}{n}} &lt; b^{\frac{1}{n}}\) There's
also of course \(a^\frac{1}{2} \equiv \sqrt{a}\) and all that comes with
it.</p>

<h3 id="modulus-or-absolute-function">Modulus or absolute function</h3>

<p>The <strong>Modulus or Absolute</strong> of \(x\), \(|x|\) is basically \(x\) but without
negatives. \(\|x\| = \sqrt{x^2}, \quad \forall x \in \mathbb{R}\).</p>

<p>There are 4 properties of modulus:</p>

<ol>
  <li>
\[-\|x\| \leq x \leq \|x\|\]
  </li>
  <li>
\[\|xy\| = \|x\|\|y\|\]
  </li>
  <li>
\[\|x + y\| \leq \|x\|+\|y\|\]
  </li>
  <li>
\[\|\|x\| - \|y\|\| \leq \|x - y\|\]
  </li>
</ol>

<h3 id="bounds">Bounds</h3>

<p>For a set of real numbers \(S\)</p>

<ul>
  <li>\(u\) is an <strong>upper bound</strong> of S if \(u \geq x \forall x \in S\)</li>
  <li>\(U\) is the <strong>least upper bound (supremum)</strong> of S if \(U\) is an UB of
S and \(U \leq u \forall u\)</li>
  <li>\(l\) is a <strong>lower bound</strong> of S if \(l \leq x \forall x \in S\)</li>
  <li>\(L\) is the <strong>greatest lower bound (infimum)</strong> of S if \(L\) is a LB of
S and \(L \geq l \forall l\)</li>
</ul>

<p><img src="./images/bounds.svg" alt="boundsDiagram" /></p>

<h4 id="completeness-axiom">Completeness axiom</h4>

<p>The <em>completeness axiom</em> suggests that for every set which has a lower
bound has a greatest lower bound.</p>

<p><strong>Important consequences of the completeness axiom:</strong></p>

<ul>
  <li><strong>The ARCHIMEDIAN PROPERTY</strong> of \(\mathbb{R}\):<br />
if \(\epsilon \in \mathbb{R}; \epsilon &gt; 0\) then
\(\exists n \in \mathbb{Z}^+ : n\epsilon &gt; 1\).</li>
  <li>Between any two real numbers there are both rational and irrational
numbers.</li>
  <li>Every real number can be represented by a (possibly infinite)
decimal expansion.</li>
</ul>

<h2 id="complex-numbers">Complex Numbers</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part1/note3.pdf">Link to the PDF.</a></p>

<p>In the set \(\mathbb{C}\), of the form \(a + ib\) where \(i^2 = -1\).
Often denoted \(z\).</p>

<p>Representable by an ordered pair \(z = (a, b)\) where \(Re_z = a\) and
\(Im_z = b\). Also representable as a point on a 2d plane, the <em>complex
plane</em> or <em>argand diagram</em>.</p>

<p>For \(a, b \in \mathbb{R}\) the <strong>complex conjugate</strong> of \(z = a + ib\) is
\(\overline{z} = a - ib\) (overline). Effectively reflecting along x axis.</p>

<h3 id="complex-conjugates">Complex conjugates</h3>

<p><strong>Properties of complex conjugates (\(\forall z, w \in \mathbb{C}\)):</strong></p>

<ul>
  <li>
\[\overline{z + w} = \overline{z} + \overline{w}\]
  </li>
  <li>
\[\overline{zw} = \overline{z}\overline{w}\]
  </li>
  <li>
\[\overline{(\frac{z}{w})} = \frac{\overline{z}}{\overline{w}}\]
  </li>
  <li>
\[\overline{\overline{z}} = z\]
  </li>
  <li>
\[z \in \mathbb{R} \Longleftrightarrow \overline{z} = z\]
  </li>
  <li>
\[Re_z = \frac{z + \overline{z}}{2}\]
  </li>
  <li>
\[Im_z = \frac{z - \overline{z}}{2i}\]
  </li>
</ul>

<h3 id="polar-coordinates">Polar coordinates</h3>

<p>If \(x, y \in \mathbb{R}\) and \(x + iy \neq 0\) we can express \(x\) and \(y\)
in polar coordinates,</p>

\[\begin{align}  &amp; x = r \cos(\theta)  &amp; y = r
\sin(\theta) \end{align}\]

\[\therefore x + iy = r(\cos(\theta) + i\sin(\theta).\]

\[\begin{align}  &amp; r
= \sqrt{x^2 + y^2}  &amp; \theta \textrm{ satisfies } \tan(\theta) =
\frac{y}{x}. \end{align}\]

<p>\(\theta\) is the <strong>argument</strong>, with the
<strong>principal argument</strong> being a \(\theta \in (-\pi, \pi]\).</p>

<p>You can multiply two complex numbers in polar form,
\(r_{1} (\cos(\theta) + i\sin(\theta)) \cdot r_{2} (\cos(\phi) + i\sin(\phi)) = r_{1}r_{2}(\cos(\theta + \phi) + i\sin(\theta + \phi))\).</p>

<h3 id="complex-modulus">Complex modulus</h3>

<p>\(r\) is the <strong>modulus</strong>, often denoted \(\|x + iy\|\).</p>

<p><strong>Properties of modulus</strong>, for all \(z, w \in \mathbb{C}\):</p>

<ul>
  <li>
\[\|z\| = \|\overline{z}\|\]
  </li>
  <li>
\[\|z\| = \sqrt{z\overline{z}}\]
  </li>
  <li>
\[z\overline{z} = \|z\|^2\]
  </li>
  <li>
\[\|zw\| = \|z\|\|w\|\]
  </li>
  <li>\(\|z + w\| \leq \|z\| + \|w\|\), <em>(triangle inequality)</em></li>
  <li>
\[\|\|z\| - \|w\|\| \leq \|z - w\|\]
  </li>
</ul>

<p>Proof of the triangle inequality is omitted.</p>

<h3 id="de-moivres-theorem">De Moivre's Theorem</h3>

<p>\(\forall n \in \mathbb{Z}\):
\((r(\cos{\theta} + i\sin \theta))^n = r^{n}(\cos{n\theta} + i\sin{n\theta}).\)</p>

<p><strong><em>Q.</em></strong> Find all complex numbers \(z : z^3 = 1\).</p>

<h3 id="fundamental-theorem-of-algebra">Fundamental Theorem of Algebra</h3>
<p>(Gauss) states that an \(n\)
degree polynomial must have \(n\) roots.</p>

<h3 id="other-notation">Other Notation</h3>

<p>The conjugate \(\overline{z}\) can also be written \(z*\).</p>

<p>Sometimes (often in engineering) \(i\) is instead written as \(j\), as \(i\) often refers to current.</p>

            <br/>
            <hr style="
                padding: 5px;
                border-radius: 1em;
                background-color: whitesmoke;
            ">
            <br/><h1 id="vectors">Vectors</h1>
       
            <h2 id="basics">Basics</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note4.pdf">Link to the PDF.</a></p>

<p>Vectors in 2 and 3D are members of the sets \(\mathbb{R}^2\) and
\(\mathbb{R^3}\) respectively. <em>(Note, thus ordered pairs)</em></p>

<p>Can be treated as coordinate points.</p>

<p>Denoted either with arrow (\(\vec{x}\)), underline (\(\underline{x}\)), or
bold (\(\mathbf{x}\)). I'll be using the arrow.</p>

<h3 id="addition-and-scalar-multiplication">Addition and scalar multiplication</h3>

<p>Addition and scalar multiplication are done element-wise. For
\(\vec{x} = (x_1, x_2, ..., x_n)\) and \(\vec{y} = (y_1, y_2, ..., y_n)\)</p>

<ul>
  <li>
\[\lambda \vec{x} = (\lambda x_1, \lambda x_2, ..., \lambda x_n)\]
  </li>
  <li>
\[\vec{x}+\vec{y} = (y_1 + x_1, y_2 + x_2, ...,y_3 + x_n)\]
  </li>
</ul>

<p>\(\vec{x} - \vec{y}\) and \(-\vec{x}\) are also defined accordingly from
these.</p>

<p>In \(\mathbb{R}^2\) if \(\vec{p} = (p_1, p_2)\) then this is the directed
line segment \(\overrightarrow{OP}\) starting at origin \(O\) and ending at
point P \((p_1, p_2)\). \(\vec{p}\) is then the <strong>position vector</strong> of P.</p>

<h3 id="position-unit-and-zero-vectors-and-vector-length">Position, unit and zero vectors, and vector length</h3>

<p>Two line segments are equivalent if they have the same length and
direction.</p>

<p>For points \(A, B\) with vectors \(\vec{a}, \vec{b}\) then
\(\overrightarrow{AB} = \vec{b} - \vec{a}\), for \(\vec{a} = (a_1, a_2) \in \mathbb{R}^2\)</p>

<p>The <strong>length of \(\vec{a}\)</strong> is written as \(\|\vec{a}\| = \sqrt{a_{1}^{2} + a_{2}^{2}}\);
similarly in 3D.</p>

<p>A <strong>unit vector</strong> has length 1. The <strong>distance</strong> between
\(\vec{a}, \vec{b} = \|\vec{b} - \vec{a}\|\).</p>

<p>The <strong>zero vector</strong>, \(\vec{0}\) has all zeros in it.</p>

<h3 id="the-scalardot-product">The scalar/dot product</h3>

<p>The <strong>scalar (dot) product</strong>, \(\vec{a} \cdot \vec{b}\) is the real number
\(a_1 b_1 + a_2 b_2 + ... + a_n b_n\).</p>

<p>The angle between two position vectors, \(\theta\) between vectors
\(\vec{a}, \vec{b}\) is given by
\(\cos \theta = \frac{\vec{a} \cdot \vec{b} }{|\vec{a}||\vec{b}|}.\) Two
vectors are <strong>orthogonal</strong> (perpendicular) if dot product is 0.</p>

<p>All definitions (if not already) can be extended to \(n\) dimensions.</p>

<h2 id="linear-combinations-and-subspaces">Linear Combinations and Subspaces</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note5.pdf">Link to the PDF.</a></p>

<h3 id="linear-combinations">Linear Combinations</h3>

<p>If \(\vec{u}_1, \vec{u}_2, ..., \vec{u}_m \in \mathbb{R}^n\) and
\(a_1, a_2, ..., a_m \in \mathbb{R}\), then any vector of the form
\(a_1 \vec{u}_1 + a_2 \vec{u}_2 + ... + a_m \vec{u}_m\) is a <strong>linear
combination</strong> of \(\vec{u}_1, \vec{u}_2, ..., \vec{u}_m\).</p>

<p>A linear combination of a single vector is defined as a multiple of that
vector.</p>

<p>In \(\mathbb{R}^3\) if \(\vec{u}, \vec{v}\) are not parallel, then
\(\alpha \vec{u} + \beta \vec{v}\) represents the vertex of a
parallelogram having \(\alpha \vec{u}, \beta \vec{v}\) as sides - a vector
in the <strong>plane</strong> containing \(\vec{u}, \vec{v}, \vec{0}\).</p>

<h3 id="spans">Spans</h3>

<p>If \(U = \{\vec{u}_1, \vec{u}_2, ..., \vec{u}_m\}\) is a
finite set of vectors in \(\mathbb{R}^n\), then the <strong>span</strong> of U is the
set of all linear combinations of vectors in U and is denoted
\(\textrm{span } U\);
\(\textrm{span } U = \{a_1 \vec{u}_1 + a_2 \vec{u}_2 + ... + a_m \vec{u}_m : a_1, a_2, ..., a_n \in \mathbb{R}\}.\)</p>

<ul>
  <li>If \(U = \{\vec{u}\}\) then the span is the set of all multiples of
\(\vec{u}\).</li>
  <li>Note that for <em>basis</em> spans, 1 vector is a line, 2 vectors is a
plane, and onwards to hyperplanes. (Basis is covered in next
section)</li>
  <li>Elementary spans of \(\mathbb{R}^2, \mathbb{R}^3\) are
\(\{(1, 0), (0, 1)\}\) and \(\{(1, 0, 0), (0,1,0), (0,0,1)\}\)
respectively.</li>
</ul>

<h3 id="subspaces">Subspaces</h3>

<p>A <strong>subspace</strong> of \(\mathbb{R}^n\) is a non-empty subset
\(S \subseteq \mathbb{R}^n\) such that:</p>

\[\begin{align} (1)  &amp; \vec{u},
\vec{v} \in S \implies \vec{u} + \vec{v} \in S;\\ (2)  &amp; \vec{u}
\in S, \lambda \in \mathbb{R} \implies \lambda \vec{u} \in S.
\end{align}\]

<p>i.e. closed on addition and multiplication.</p>

<p>Means if a set of vectors is in a subspace, any linear combinations of
those vectors is also in.</p>

<p>Two elementary subspaces of \(\mathbb{R}^n\) are \(\{\vec{0}\}\) (just
empty) and \(\mathbb{R}^n\) itself.</p>

<h4 id="properties-of-subspaces">Properties of Subspaces</h4>

<ol>
  <li>Every subspace contains \(\vec{0}\).</li>
  <li>If \(U\) is a nonempty finite subset of \(\mathbb{R}^n\) then
\(\textrm{span } U\) is a subspace, the subspace <strong>spanned or
generated</strong> by U.</li>
</ol>

<h3 id="exercise">Exercise</h3>

<p>Determine if \(S\) is a subspace of \(\mathbb{R}^n\):</p>

<ol>
  <li>
\[S = \{(x, y, 0) : x, y \in \mathbb{R}\} \in \mathbb{R}^3\]
  </li>
  <li>
\[S = \{(1, 1)\} \in \mathbb{R}^2\]
  </li>
  <li>
\[S = \{(x, y) : x^2 + y^2 \leq 1\} \in \mathbb{R}^2\]
  </li>
</ol>

<p><strong><em>Solutions.</em></strong></p>

<p>1. We need to show closure on addition and scaling. Let
\(\vec{u}, \vec{v} \in S : \vec{u} = (a, b, 0), \vec{v} = (c, d, 0)\) for
some \(a, b, c, d \in \mathbb{R}\).
\(\vec{u} + \vec{v} = (a, b, 0) + (c, d, 0) = (a+c, b+d, 0) \in S.\) For
any \(\lambda \in \mathbb{R}\)
\(\lambda \vec{u} = \lambda (a, b, 0) = (\lambda a, \lambda b, 0) \in S.\)</p>

<p>2. Nope, since \(2(1,1) \not \in S\), so no scaling closure.</p>

<p>3. Nope. Let \(\vec{u} = (1, 0), \vec{v} = (0, 1)\), both of which
\(\in S\), however \(\vec{u} + \vec{v} = (1, 1)\).
\(1^2 + 1^2 = 2 \not \leq 1\), so not closed under addition.</p>

<h2 id="linear-independence">Linear Independence</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note5.pdf">Link to the PDF.</a></p>

<p>A set of vectors
\(\{\vec{u}_1, \vec{u}_2, ..., \vec{u}_m\} \in\mathbb{R}^n\) are
<strong>linearly <em>dependent</em></strong> IF there are real numbers \(a_1, a_2, ..., a_n\)
which are NOT ALL ZERO such that
\(a_1 \vec{u}_1 + a_2 \vec{u}_2 + ... + a_m \vec{u}_m = \vec{0}.\)</p>

<p>Thus a <strong>linearly <em>independent</em></strong> set is where IF
\(a_1 \vec{u}_1 + a_2 \vec{u}_2 + ... + a_m \vec{u}_m = \vec{0}.\) THEN
ALL \(a_i\) are <strong>0</strong>.</p>

<p>i.e. if you can't find a nonzero linear combination that makes zero
vector, then the set is linearly independent.</p>

<ul>
  <li>If a set contains one nonzero vector, it is linear independence</li>
  <li>If it contains the zero vector, it is linear dependence</li>
</ul>

<p>In a set of three vectors, you can fairly easily solve three simultaneous
equations all with the sum of zero. Then, you either find that your
three coefficients \(\alpha, \beta, \gamma\) has to be 0 (indep) or there
is some non-zero relationship between at least two of them (dep)</p>

<p><strong><em>Theorem.</em></strong> A set \(\{\vec{u}_1, \vec{u}_2, ..., \vec{u}_m\}\) of
nonzero vectors is linearly <em>depending</em> <strong>iff</strong> some / any vector
\(\vec{u}_r\) is a linear combination of its predecessors $${\vec{u}_1,
\vec{u}_2, ..., \vec{u}_m{r-1}}$$</p>

<p><em>(proof omitted)</em></p>

<h2 id="basis-and-dimension">Basis and Dimension</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note7.pdf">Link to the PDF.</a></p>

<h3 id="basis">Basis</h3>

<p>Let \(S\) be a subspace of \(\mathbb{R}^n\). A set of vectors is a <strong>basis</strong> of S if it is a <em>linearly independent</em> set which spans S.</p>

<p>e.g. The set \(\{(1,0,0), (0,1,0), (0,0,1)\}\) is a basis for
\(\mathbb{R}^3\). In fact, it is the <strong>standard basis</strong>.</p>

<p><strong><em>Theorem.</em></strong> Let \(S\) be subspace of \(\mathbb{R}^n\). If a set
\(\vec{v}_1, \vec{v}_2, ..., \vec{v}_m\) spans S, then any <em>linearly
independence</em> subset of S has <em>at most</em> \(m\) vectors.</p>

<p><em>(proof omitted)</em></p>

<p>This leads to the fact that any two bases for a subpace S have the
<strong>same</strong> number of elems.</p>

<h3 id="dimension">Dimension</h3>

<p>The <strong>dimension</strong> of a subspace of \(\mathbb{R}^n\) is
the number of vectors in the basis.</p>

<h4 id="exercises">Exercises</h4>

<p><strong>1.</strong> Show that the set \(S = \{(x, y, z) : x + 2y - z = 0\}\) is
a subspace of \(\mathbb{R}^3\), and find a basis and dimension of \(S\).</p>

<p><strong><em>Solution.</em></strong> We can rewrite S as:</p>

\[\begin{align} S  &amp; = \{(x, y, x+2y =
0) : x, y \in \mathbb{R}\} \\  &amp; = \{x(1, 0, 1) + y(0, 1, 2) : x, y
\in \mathbb{R}\} \\  &amp; = \textrm{ span } \{(1, 0, 1), (0, 1, 2)\}.
\end{align}\]

<p>Whih shows that S is a subspace, since the span of any
nonempty finite set is a subspace (known property).</p>

<p>By inspection we can see that the two vectors in the set are
independent, thus it is a basis. Thus the dimension of S is 2.</p>

<p>(Supposedly) we can use the theorem from <a href="#vec-3">linear independence</a>
to construct a basis from as panning set.</p>

<p>Let \(\{\vec{v}_1, \vec{v}_2, ..., \vec{v}_m\}\) be a basis of a subspace
S of \(\mathbb{R}^n\). Then removing each \(\vec{v}_i\) which is a linear
combination of its "predecessors" will leave a basis for S.</p>

<p><strong>2.</strong> Find a basis for and dimension of a subspace S (of
\(\mathbb{R}^4\)) spanned by
\(\{(2,1,0,-3), (-1,0,-1,2), (1,2,-3,0), (0,0,0,1), (0,1,-2,0)\}.\)</p>

<p><strong><em>Solution.</em></strong> Let's look at (1, 2, -3, 0) and see if it is a linear comb.
of predecessors. \((1, 2, -3, 0) = \alpha(2,1,0,-3) + \beta(-1,0,-1,2)\)</p>

\[\begin{align}  &amp; \implies \begin{cases} 2\alpha - \beta  &amp; = 1 \\
\alpha  &amp; = 2 \\ -\beta  &amp; = -3 \\ -3\alpha + 2\beta  &amp; = 0
\end{cases}  &amp; \implies \begin{cases} \alpha  &amp; = 2 \\ \beta  &amp; = 3.
\end{cases} \end{align}\]

<p>We can see it is a linear combination, so we
remove, giving \(\{(2,1,0,-3), (-1,0,-1,2), (0,0,0,1), (0,1,-2,0)\}.\)</p>

<p>Now we next check \((0,0,0,1)\).
\((0,0,0,1) = \alpha(2,1,0,-3) + \beta(-1,0,-1,2)\)</p>

\[\begin{align}
 &amp; \implies \begin{cases} 2\alpha - \beta  &amp; = 0 \\ \alpha  &amp; = 0 \\
-\beta  &amp; = 0 \\ -3\alpha + 2\beta  &amp; = 1 \end{cases}  &amp; \implies
\begin{cases} \alpha  &amp; = 0 \\ \beta  &amp; = 0 \\ -3\alpha + 2\beta  &amp; = 1 \end{cases} \end{align}\]

<p>Which have no solution of \(\alpha, \beta\)
and thus (0, 0, 0, 1) is not a linear combination of priors. Thus
\(\{(0,0,0,1),(2,1,0,-3),(-1,0,-1,2)\}\) is a linear independence set.</p>

<p>Check the last one against all others,
\((0,1,-2,0) = \alpha(2,1,0,-3) + \beta(-1,0,-1,2) + \gamma(1, 2, -3, 0)\)</p>

\[\begin{align}  &amp; \implies \begin{cases} 2\alpha - \beta  &amp; = 0 \\
\alpha  &amp; = 1 \\ \beta  &amp; = 2 \\ -3\alpha + 2\beta + \gamma  &amp; = 0
\end{cases}  &amp; \implies \begin{cases} \alpha  &amp; = 1 \\ \beta  &amp; = 2
\\ \gamma  &amp; = -1. \end{cases} \end{align}\]

<p>So we remove that, thus
finally the remaining set \(\{(0,0,0,1),(2,1,0,-3),(-1,0,-1,2)\}\) is the
final basis of \(S\), which gives \(S\) a dimension of 3.</p>

<h3 id="properties-of-bases">Properties of bases</h3>

<p>Let S be an \(m\)-dimensional subspace of \(\mathbb{R}^n\) then</p>

<ol>
  <li>Any subset of S with more than \(m\) vectors is linearly <em>dependent</em>;</li>
  <li>A subset of S is a basis <em>if and only if</em> it is a linearly
independent set containing \(m\) vectors.</li>
</ol>

<p>It then follows that any subset of \(\mathbb{R}^n\) is linearly
<em>dependent</em>, and a subset of \(\mathbb{R}^n\) <em>iff</em> it is a linearly
independent set containing \(n\) vectors.</p>

<p>\(\mathbb{R}^2, \mathbb{R}^3\) properties...</p>

<p>Subspaces of \(\mathbb{R}^2\):</p>

<ol>
  <li>There is one 0 dim subspace \(\{\vec{0}\}\)</li>
  <li>A 1D subspace is spanned by a single non-zero vector: straight lines
through origin.</li>
  <li>The only 2D subspace is \(\mathbb{R}^2\)</li>
</ol>

<p>Subspaces of \(\mathbb{R}^3\):</p>

<ol>
  <li>There is one 0 dim subspace \(\{\vec{0}\}\)</li>
  <li>A 1D subspace is spanned by a single non-zero vector: straight lines
through origin.</li>
  <li>A 2D subspace is spanned by 2 linear independence vectors: plains containing
the origin</li>
  <li>The only 3D subspace is \(\mathbb{R}^3\)</li>
</ol>

<h3 id="change-of-basis">Change of basis</h3>

<p><strong><em>Exercise</em></strong></p>

<p>What are the co-ordinates of the vector \(\vec{w} = (8,-9,6)\) in relation to the basis \(V = \{(1,-1,3),(-3,4,9),(2,-2,4)\}\)</p>

<p><strong><em>Solution #1</em></strong>
\(\begin{align}
	\begin{cases}
		\alpha - 3\beta + 2\gamma  &amp; = 8 \\
		-\alpha + 4\beta -2\gamma  &amp; = -9 \\
		3\alpha + 9\beta + 4\gamma  &amp; = 6
	\end{cases}
	&amp; \implies
	\begin{cases}
		\alpha  &amp; = 5 \\
		\beta  &amp; = -1 \\
		\gamma  &amp; = 0.
	\end{cases}
\end{align}\)
Hence, the co-ordinates in the new basis are \(\vec{w} = (5,-1,0)\)</p>

<p><strong><em>Solution #2</em></strong>
\(\alpha \vec{v}_1 + \beta \vec{v}_2 + \gamma \vec{v}_3 = \begin{bmatrix}8 \\ -9 \\ 6\end{bmatrix}\)</p>

\[x\begin{bmatrix}1 \\ -1 \\ 3\end{bmatrix} + y\begin{bmatrix}-3 \\ 4 \\ 9\end{bmatrix} + z\begin{bmatrix}2 \\ -2 \\ 4\end{bmatrix} = \begin{bmatrix}8 \\ -9 \\ 6\end{bmatrix}\]

\[\implies x = 5, y = -1, z = 0\]


            <br/>
            <hr style="
                padding: 5px;
                border-radius: 1em;
                background-color: whitesmoke;
            ">
            <br/><h1 id="matrices">Matrices</h1>
       
            <h2 id="matrix-algebra">Matrix Algebra</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note8.pdf">Link to the PDF.</a></p>

<p>Matrices are rectangular arrays of elements. It's order is its
\(\textrm{row } \times \textrm{ column}\). Note that an \(m \times 1\)
matrix is called a <strong>column matrix/vector</strong>, and \(1 \times n\) matrices
are similarly named as rows. Elements are referred to with subscript row
column: \(a_{ij}\).</p>

<h3 id="addition-and-scalar-multiplication">Addition and scalar multiplication</h3>

<p>Sum of two matrices is only defined if they have the same order, and is
<em>elementwise</em> addition.</p>

<p>Scalar multiplication is also done elementwise.</p>

<h4 id="properties-of-addition-and-scalar-multiplication">Properties of addition and scalar multiplication</h4>

<p>\(\forall m \times n\) matrices \(A, B, C\),
\(\forall \lambda, \mu \in \mathbb{R}\):</p>

<ol>
  <li>\(A + (B+C) = (A+B)+C\) (associativity of addition)</li>
  <li>
\[A + O = A = O + A\]
  </li>
  <li>
\[A + (-A) = O = (-A) + A\]
  </li>
  <li>\(A+B=B+A\) (commutativity of addition)</li>
  <li>
\[(\lambda + \mu)A = \lambda A + \mu A\]
  </li>
  <li>
\[\lambda (A+B) = \lambda A + \lambda B\]
  </li>
  <li>
\[\lambda(\mu A) = (\lambda \mu ) A\]
  </li>
</ol>

<h3 id="matrix-multiplication">Matrix multiplication</h3>

<p>Matrix multiplication can only happen between an
\(A_{m \times n}\) and a \(B_{n \times p}\) (note the
highlighted dimensions) and will produce a matrix \(C_{m \times p}\).</p>

<p>Matrix multiplication is hard to explain in text, so <a href="https://www.youtube.com/watch?v=as8C8w-Nz94">see this
video</a> by blackpenredpen if you're not sure</p>

<h4 id="properties-of-matrix-multiplication">Properties of matrix multiplication</h4>

<p>Whenever the products exist, matrix multiplication has the properties:</p>

<ol>
  <li>\((AB)C = A(BC)\) (associativity)</li>
  <li>\(A(B+C) = AB + AC\), \((A+B)C = AC + BC\)</li>
  <li>
\[IA = A = AI\]
  </li>
  <li>
\[OA = O = AO\]
  </li>
  <li>\(A^p A^q = A^{p+q} = A^q A^p\), \((A^p)^q = A^{pq}\)</li>
</ol>

<p>Note that matrix multiplication is <strong>not commutative</strong>: \(AB \neq BA\)
(for all but specific circumstances).</p>

<h3 id="matrix-transposition">Matrix transposition</h3>

<p>The <strong>transpose</strong> \(A^T\) of a matrix is obtained by swapping rows and
columns (i.e. reflecting on leading diagonal).</p>

<h4 id="properties-of-transposition">Properties of transposition</h4>

<ol>
  <li>\((A^T)^T=A\) holds for any matrix \(A\)</li>
  <li>\((A+B)^T = A^T + B^T\) if \(A+B\) exists</li>
  <li>\((\lambda A)^T = \lambda A^T\) for any \(\lambda \in \mathbb{R}\)</li>
  <li>\((AB)^T = B^T A^T\) if \(AB\) exists.</li>
</ol>

<p>For same order square matrices \(A, B\), \(B\) is the inverse of \(A\) if and
only if \(AB = I = BA\). The inverse (should it exist) is <strong>unique</strong> and
denoted \(A^{-1}\).</p>

<h3 id="types-of-matrices">Types of matrices</h3>

<p>A <strong><em>square matrix</em></strong> \(A_{n \times n}\) is said to be of <em>order \(n\)</em></p>

<p>The <strong><em>zero matrix</em></strong>, denoted \(O_{m \times n}\) is a matrix of all zeros.</p>

<p><strong>Diagonal matrices</strong> only have elements on the leading diagonal;
\(a_{ii}\) for some \(i : [1..n]\).</p>

<p>The <strong>identity matrix</strong> \(I\) (or \(I_n\)) is the \(n \times n\) diagonal
matrix whose diagonal elements are all 1.</p>

<p>For a square matrix \(A\), \(A, AA, AAA, ...\) are defined as
\(A, A^2, A^3,...\) respectively. \(A^0 = I\). Functions
\(\exp(A), \cos(A), \sin(A)\) can also be defined <em>(hint: taylor series)</em>.</p>

<h3 id="determinant-of-a-2x2-matrix">Determinant of a 2x2 matrix</h3>

<p>The <strong>determinant</strong> of a \(2 \times 2\) matrix
\(A = \begin{bmatrix} a  &amp;  b \\ c  &amp;  d \end{bmatrix}\) is \(ad-bc\) and
denoted \(|A|, \det(A)\).</p>

<p>If a \(2 \times 2\) matrix is invertible, then
\(\det(A)det(A^{-1}) = \det(AA^{-1}) = \det(I) = 1\). Thus
\(\det(A) \neq 0\) and in that case,</p>

<p>The inverse of \(A = \begin{bmatrix} a  &amp;  b \\ c  &amp;  d \end{bmatrix}\) is
\(A^{-1} = \frac{1}{\det A} \begin{bmatrix} d  &amp;  -b \\ -c  &amp;  a \end{bmatrix}\)
(a particular case of a general result)</p>

<h2 id="matrix-inverse-linear-equations">Matrix Inverse, Linear Equations</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note9.pdf">Link to the PDF.</a></p>

<p>A system of linear equations can be written in matrix form.</p>

\[\begin{align} ax_1 + bx_2  &amp; = y_1 \\ cx_1 + dx_2  &amp; = y_2
\end{align}\]

<p>\(\equiv \begin{bmatrix} a  &amp;  b \\ c  &amp;  d \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \end{bmatrix}.\)
Which can be extrapolated to general form.</p>

<h3 id="elementary-row-operations">Elementary row operations</h3>

<p>The following operations can be performed to solve a system (<strong>Gaussian
Elimination</strong>):</p>

<ul>
  <li>Swap two rows (equations)</li>
  <li>Multiply a row (both sides of an equation) by a nonzero number</li>
  <li>Add a multiple of one row (equation) to another</li>
</ul>

<h3 id="augmented-matrices">Augmented matrices</h3>

<p>Which can be done over the <em>augmented matrix</em>, which is gotten by
combining \(\begin{bmatrix} a  &amp;  b \\ c  &amp;  d \end{bmatrix}\) and
\(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix}\) (the coefficients and the
result).
\(\left[\begin{array}{cc|c} a  &amp;  b  &amp;  y_1 \\ c  &amp;  d  &amp;  y_2 \end{array}\right]\)</p>

<p>Two matrices \(A, B\) are <strong>row equivalent</strong> if we can use row operations
to get from A to B. Denoted \(A \sim B\).</p>

<p>A matrix is in <strong>row echelon form</strong> if the first nonzero entry in each
row is further to the right of said entry in the previous row. By
reducing to row echelon form we can solve a system of linear equations.</p>

<p>See the pdf for sample problems. <em>Note: there also exists <strong>reduced row
echelon form</strong>, where each leading entry is a 1, and each column with a
1 in has 0s for all other entries.</em></p>

<p>Elementary row operations can be done by multiplying by so-called <em>elementary
matrices</em>. These are defined for (\(E_{n \times n}\)):</p>

<ul>
  <li>\(E_{ij}\) obtained from \(I\) by exchanging rows \(i, j\)</li>
  <li>For \(\lambda \neq 0, \; E_i(\lambda)\) obtained from \(I\) by
multiplying row \(i\) by \(\lambda\)</li>
  <li>\(E_{ij}(\mu)\) obtained from \(I\) by adding \(\mu \cdot\) row \(j\) to row
\(i\)</li>
</ul>

<p>Every elementary matrix is invertible.</p>

<p>If a sequence of row operations transforms a square matrix \(A\) into \(I\).
then \(A^{-1}\) exists and the same sequence transforms \(I\) into \(A\).</p>

<p>This is best done with an augmented matrix, like
\(\left[\begin{array}{cc|cc} a  &amp;  b  &amp;  1  &amp;  0 \\ c  &amp;  d  &amp;  0  &amp;  1 \end{array}\right]\)</p>

<h2 id="matrix-inverse-determinants">Matrix Inverse, Determinants</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note10.pdf">Link to first PDF.</a> 
<a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note11.pdf">Link to second PDF.</a></p>

<h3 id="determinant-of-a-3x3-matrix-and-the-cofactor-matrix">Determinant of a 3x3 matrix and the cofactor matrix</h3>

<p>The determinant of a \(3 \times 3\) matrix is denoted the same way, and is
defined</p>

\[\begin{vmatrix} a_{11}  &amp;  a_{12}  &amp;  a_{13} \\ a_{21}  &amp; 
a_{22}  &amp;  a_{23} \\ a_{31}  &amp;  a_{32}  &amp;  a_{33} \end{vmatrix} =
a_{11} \begin{vmatrix} a_{22}  &amp;  a_{23} \\ a_{32}  &amp; 
a_{33}\end{vmatrix} - a_{12} \begin{vmatrix}a_{21}  &amp;  a_{23} \\
a_{31}  &amp;  a_{33}\end{vmatrix} + a_{32} \begin{vmatrix}a_{21}  &amp; 
a_{22} \\ a_{31}  &amp;  a_{32}\end{vmatrix}\]

<p>i.e. all elements on
the first row multiplied (respecting +/- grid) with the determinant of
the minor matrix (the cofactor) - the matrix gotten by deleting the row
and column with said element.</p>

\[\begin{bmatrix}+ &amp; - &amp; + \\ - &amp; + &amp; - \\ + &amp; - &amp; +\end{bmatrix}\]

<p>This can be done with any row or column.</p>

<h3 id="elementary-row-operations-on-determinants">Elementary row operations on determinants</h3>

<p>On elementary row operations and determinants (\(B\) obtained from \(A\)):</p>

<ol>
  <li>Multiplying a row in A by a \(\lambda\): \(\|B\| = \lambda\|A\|\)</li>
  <li>Swapping 2 rows of A: \(\|B\| = -\|A\|\)</li>
  <li>Adding a multiple of one row to another: \(\|B\| = \|A\|\)</li>
</ol>

<h3 id="cramers-rule-to-invert-matrices">Cramer’s rule to invert matrices</h3>

<p>A square matrix is inversible <em>iff</em> its determinant is not 0. If \(A\) is
invertible, \(A^{-1} = \frac{1}{|A|} \textrm{adj}(A)\) where
\(\textrm{adj}(A)\) is the <strong>transposed matrix of cofactors</strong>.</p>

<p>You can also use matrix inverses to calculate equations:</p>

\[\begin{align}
\textrm{if }  &amp; A\vec{x} = \vec{y}\\ \textrm{then }  &amp; A^{-1} A
\vec{x} = A^{-1} \vec{y} \implies \vec{x} = A^{-1}\vec{y}
\end{align}\]

<p>Where the column vector x are the variables, and column
vector y are the values of the equations.</p>

<h3 id="linear-independence-via-determinants">Linear independence via determinants</h3>

<p>A set of \(n\) vectors in
\(\mathbb{R}^n\) is linearly independent <em>if and only if</em> it is the set of
column vectors of a matrix with nonzero determinant.</p>

<p>Basically, bang \(n\) \(\mathbb{R}^n\) vectors into a square matrix, compute
the determinant, and if it is 0, then those vectors are linearly
dependent.</p>

<h2 id="linear-transformations">Linear Transformations</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note12.pdf">Link to the PDF.</a></p>

<p>A function \(T : \textrm{R}^m \longrightarrow \mathbb{R}^n\) is a <strong>linear
transformation</strong> if,
\(\forall \vec{u}, \vec{v} \in \mathbb{R}^n, \lambda \in \mathbb{R}\), we
have: \(T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v}),\)
\(T(\lambda \vec{u}) = \lambda T(\vec{u}).\) Which are preservation of
addition and scaling respectively. Also,</p>

\[T(\vec{0}) = \vec{0}.\]

<p>For simple problems, verifying that the transformation fits the two
rules of addition and scaling is sufficient.</p>

<p><strong><em>Example.</em></strong> Let \(\vec{u}\) be a nonzero 2D vector. If
\(\vec{x} \in \mathbb{R}^2\) then we define the <strong>projection</strong> of
\(\vec{x}\) onto \(\vec{u}\) to be a vector \(P_{\vec{u}}(\vec{x})\) such that</p>

<ol>
  <li>\(P_{\vec{u}}(\vec{x})\) is a multiple of \(\vec{u}\)</li>
  <li>\(\vec{x} - P_{\vec{u}}(\vec{x})\) is perpendicular to \(\vec{u}\).</li>
</ol>

<p>We have by (1) that \(P_{\vec{u}}(\vec{x}) = \alpha \vec{u}\) for some
\(\alpha \in \mathbb{R}\), so by (2)
\(0 = (\vec{x} - P_{\vec{u}}(\vec{x})) \cdot \vec{u} = (\vec{x} - \alpha \vec{u}) \cdot \vec{u} = \vec{x} \cdot \vec{u} - \alpha |\vec{u}|^2,\)
\(\implies \alpha = \frac{\vec{x}\cdot\vec{u}}{|\vec{u}|^2}.\)</p>

<p>The projection can then be regarded as a function
\(P_\vec{u} : \mathbb{R}^2 \longrightarrow \mathbb{R}^2\) defined
\(\forall \vec{x} \in \mathbb{R}^2\):
\(P_{\vec{u}}(\vec{x}) = (\frac{\vec{x}\cdot\vec{u}}{|\vec{u}|^2})\vec{u}.\)
This function can be verified to be a linear transformation.</p>

<p><strong><em>Example.</em></strong> For \(\theta \in [0, 2\pi)\) define
\(R_\theta : \mathbb{R}^2 \longrightarrow \mathbb{R^2}\) to be a function
describing rotation about angle \(\theta\) through origin. After a bit of
derivation, we get
\(R_\theta (x, y) = (x\cos\theta - y\sin\theta, x\sin\theta - y\cos\theta).\)
Or alternatively in matrix form (let \((x', y')\) be \(R_\theta (x, y)\)) as
\(\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix}\ \cos\theta  &amp;  -\sin\theta \\ \sin\theta  &amp;  \cos\theta \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}.\)</p>

<h2 id="linear-transformations-and-matrices">Linear Transformations and Matrices</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note13.pdf">Link to the PDF.</a></p>

<p>Referring back to the last example in the last section, it is further
true that <strong>every</strong> \(M_{m \times n}\) matrix can act as a linear
transformation (\(T(\vec{x}) = M\vec{x}\)). Vectors are column vectors.</p>

<p>For a basis \(V = \{\vec{v}_1, \vec{v}_2, ... \vec{v}_n\}\) of
\(\mathbb{R}^n\), every \(\vec{x} \in \mathbb{R}^n\) has a linear expansion
\(\vec{x} = a_1 \vec{v}_1 + a_2 \vec{v}_2 + ... + a_n \vec{v}_n\).</p>

<p>These coefficients \(a_1 ... a_n\) are the <strong>coordinates of \(x\) with
respect to basis \(V\)</strong>.</p>

<p>Let \(T : \mathbb{R}^m \longrightarrow \mathbb{R}^n\) be a linear
transformation, V be a basis in \(\mathbb{R}^m\) and W a basis in
\(\mathbb{R}^n\).</p>

<p>For each vector \(\vec{v}\) in V \(T(\vec{v})\) has an expansion in W. The
<strong>Matrix of a linear transformation</strong> T with respect to V and W is the
\(m \times n\) matrix where each column \(i\) contains the coefficients of
the expansion of \(T(\vec{v}_i)\) for each \(\vec{v}_i \in V\).</p>

<p>When \(m = n, \; W = V\) then it is referred to as the <strong>matrix of T with
respect to basis V</strong>.</p>

<p><strong><em>Matrix of a linear transformation.</em></strong> For a linear transformation T
(as above), M the matrix of T with respect to bases V, W (as above); the
columns of M contain the coordinates of the <em>images</em> of the basis
vectors in V w/ respect to W.</p>

<p>If \(\vec{x} \in \mathbb{R}^m\) has coordinates \([x_1,...,x_n]\) with
respect to V then the coordinates with respect to W, \([y_1, ... , y_n]\)
are
\(\begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = M \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}.\)</p>

<p>A matrix that changes between two different bases in \(\mathbb{R}^n\) is
called a <strong>transition matrix</strong>.</p>

<p>The definition on the notes is uh, just do the same thing as above but
the matrix will be square.</p>

<h2 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part2/note14.pdf">Link to the PDF.</a></p>

<p>Relating to matrices multiplying vectors, and especially where the
vectors don't change direction.</p>

<p>For a matrix A and a vector \(\vec{r}\), if
\(A\vec{r} = \lambda \vec{r}, \; \lambda \in \mathbb{R}\), then \(\vec{r}\)
is the <strong>eigenvector</strong> and \(\lambda\) is the <strong>eigenvalue</strong>.</p>

<p>A number \(\lambda\) is an eigenvalue of A if and only if it satisfies the
<strong>characteristic equation</strong> \(|A - \lambda I| = 0.\)</p>

<p>For an order \(n\) matrix there are \(n\) (not necessarily unique)
eigenvalues. Eigenvalues can also be \(\in \mathbb{C}\).</p>

<p>Recall that diagonal matrices are written
\(\textrm{diag}[a_{11}, a_{22}, ..., a_{nn}]\).</p>

<h3 id="diagonalisation-of-matrices">Diagonalisation of Matrices</h3>

<p>For an order \(n\) matrix A:
\(A = UDU^{-1}\) Where
\(D = \textrm{diag}[\lambda_1, \lambda_2, ..., \lambda_n]\) (the eigen
values), and \(U = [\vec{v}_1, \vec{v}_2, \dots, \vec{v}_n]\) are the
<em>corresponding</em> eigenvectors of said eigenvalues.</p>

<p>Note that if you have repeated eigenvalues, you have to find multiple
<em>distinct</em> (linear independence) eigenvectors for that eigenvalue.</p>

            <br/>
            <hr style="
                padding: 5px;
                border-radius: 1em;
                background-color: whitesmoke;
            ">
            <br/><h1 id="sequences-and-series">Sequences and Series</h1>
       
            <h2 id="sequences">Sequences</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part3/note15.pdf">Link to the PDF.</a></p>

<p>A sequence \((a_n)\) is an infinite list of numbers \((a_0, a_1,...)\). We
can define sequences through <em>nth term</em> rules or <em>recursively</em>.</p>

<h3 id="convergent-sequences">Convergent Sequences</h3>

<p>A sequence \(a_n\) is said to <strong>converge</strong> to a
<strong>limit</strong> \(l \in \mathbb{R}\) if for every small \(\epsilon &gt; 0\) there is
a related integer \(N\) such that \(|a_n - l| &lt; \epsilon \; \forall n &gt; N\).
This is denoted as</p>

\[\begin{align}  &amp; \lim_{n \rightarrow \infty} a_n
= l;  &amp; a_n \rightarrow l. \end{align}\]

<p>In layman's terms, if a sequence converges, then for every
(increasingly small) positive number, the difference between the
sequence's terms and the limit will be eventually smaller.</p>

<p>\(N\) here is like a <em>threshold indicator</em>, where every term of the
sequence after \(N\) will be within the \(\epsilon\) difference.</p>

<p>We generally worth with \(n \in \mathbb{N}\).</p>

<p><strong><em>Example.</em></strong> The sequence \(a_n = \frac{1}{n}\) converges to 0. We can
prove this by going to the definition - let us have a small
\(\epsilon &gt; 0\).</p>

<p>Then \(\exists N : a_N - 0 &lt; \epsilon\), or \(\frac{1}{N} &lt; \epsilon\). We
can choose any integer \(N &gt; \frac{1}{e}\), to demonstrate that this is
possible for any \(\epsilon\). For any \(n &gt; N\):
\(|\frac{1}{n} - 0| = \frac{1}{n} &lt; \frac{1}{N} &lt; \epsilon,\) to write it
out as an equation.</p>

<p>If possible, try break a large sequence down into simpler components.
They can be combined using the following:</p>

<h4 id="combination-rules-for-convergent-sequences">Combination Rules for Convergent Sequences</h4>

<p>For convergent
sequences
\(a_n \rightarrow \alpha, b+n \rightarrow \beta, c_n \rightarrow \gamma\):</p>

<ol>
  <li><strong>Sum</strong> rule: \(a_n + b_n \rightarrow \alpha + \beta\)</li>
  <li><strong>Scalar multiple</strong> rule:
\(\lambda a_n \rightarrow \lambda\alpha, \; \lambda \in \mathbb{R}\)</li>
  <li><strong>Product</strong> rule: \(a_n b_n \rightarrow \alpha\beta\)</li>
  <li><strong>Reciprocal</strong> rule: \(\frac{1}{a_n} \rightarrow \frac{1}{\alpha}\)</li>
  <li><strong>Quotient</strong> rule:
\(\frac{b_n}{a_n} \rightarrow \frac{\beta}{\alpha}\)</li>
  <li><strong><em>Hybrid</em></strong> rule:
\(\frac{b_n c_n}{a_n} \rightarrow \frac{\beta\gamma}{\alpha}\)</li>
</ol>

<h4 id="exercise">Exercise</h4>

<p>Show that the following sequence converges, find its
limit: \(a_n = \frac{(n+2)(2n-1)}{3n^2 + 1}\)</p>

<p><strong><em>Solution.</em></strong> The components do not converge themselves, a technique here is <em>dividing
by the fastest increasing term</em>. Dividing by \(n^2\),
\(a_n = \frac{\frac{(n+2)}{n}\frac{(2n-1)}{n}}{\frac{3n^2 +1}{n^2}} = \frac{(1 + \frac{2}{n})(2 - \frac{1}{n})}{3 + \frac{1}{n^2}}.\)</p>

<p>Now the components \(\frac{1}{n}\) and \(\frac{1}{n^2}\) converge to 0, so
applying combination rules we can get
\(a_n \rightarrow \frac{(1+0)(2-0)}{3+0} = \frac{2}{3}.\)</p>

<h3 id="bounded-sequences">Bounded sequences</h3>

<p>A sequence \(a_n\) is <strong>bounded above</strong> if there is a number
\(U : a_n \leq U \forall n\). \(a_n\) is <strong>bounded below</strong> if there is a
number \(L : L \leq a_n \forall n\). A sequence is <strong>bounded</strong> if it is
bounded both above and below.</p>

<h3 id="subsequences">Subsequences</h3>

<p>A <strong>subsequence</strong> of a sequence is obtained from the original sequence
by deleting some terms. We can say \(a_{2n}, a_{2n+1}\) are the even and
odd subsequences respectively of \(a_n\).</p>

<p>A sequence \(a_n\) is an <strong>increasing</strong> sequence if
\(a_{n+1} \geq a_n \;\forall n\). It is a <strong>decreasing</strong> sequence if
\(a_{n+1} \leq a_n \;\forall n\).</p>

<h3 id="basic-properties-of-convergent-sequences">Basic Properties of Convergent Sequences</h3>

<ol>
  <li>A convergent sequence has a <em>unique</em> limit.</li>
  <li>If \(a_n \rightarrow l\) then <em>every subsequence</em> of \(a_n\) also
converges to \(l\).</li>
  <li>If \(a_n \rightarrow l\) then \(\|a_n\| \rightarrow \|l\|\).</li>
  <li><strong>The squeeze rule.</strong> If
\(a_n \rightarrow l, b_n \rightarrow l; a_n &lt; c_n &lt; b_n \; \forall n\)
then \(c_n \rightarrow l\).</li>
  <li>A convergent sequence is always bounded.</li>
  <li>An increasing sequence which is bounded above converges. A
decreasing sequence which is bounded below converges.</li>
</ol>

<p>The sequence \(c_n\) is "squeezed" between two sequences which both
converge to the same limit, so naturally it will too.\(\exists B &gt; 0 : - B \leq a_n \leq B, \; \forall n\).</p>

<p>You can demonstrate an alternating sequence (e.g. \((-1)^n\)) doesn't
converge by looking at <em>subsequences</em>.</p>

<h4 id="exercise-1">Exercise</h4>

<p>Show that for \(x \geq 0, n &gt; 0\), this:
\((1+x)^{\frac{1}{n}} \leq 1 + \frac{x}{n}\).</p>

<p>Hence deduce/show that if \(c &gt; 0\) then \(c^\frac{1}{n} \rightarrow 1\).</p>

<p>Reminder: the binomial theorem is
\((1 + x)^n = 1 + nx + \frac{n(n-1)}{2!}x^2 + ... + \frac{n!}{k!(n-k)!}x^k\)</p>

<p><strong><em>Solution.</em></strong> Firstly, we can rearrange and use the binomial theorem.</p>

\[\begin{align} 1 + x  &amp; \leq (1 + \frac{x}{n})^n. \\ (1 +
\frac{x}{n})^n  &amp; = 1 + n\frac{x}{n} + \textrm{ (other positive terms)
via binomial exp, so} \\ 1 + x  &amp; \leq 1 + x + \textrm{ (other
positive terms)} \\ \end{align}\]

<p>Thus if we take \(n^\textrm{th}\) roots
of each side we will demonstrate that
\((1+x)^\frac{1}{n} \leq 1 + \frac{x}{n}.\)</p>

<p>For the second part, we need to consider separately the cases \(c \geq 1\)
and \(c &lt; 1\).</p>

<p>If \(c \geq 1\) then \(c^\frac{1}{n} \geq 1\). If we use the inequality we
demonstrated in the first part, letting \(x = c-1 \geq 0\) we get
\(1 \leq c^\frac{1}{n} \leq 1 + \frac{c-1}{n}.\)</p>

<p>By the squeeze rule, we can (in an ideal world) see that
\(c^\frac{1}{n} \rightarrow 1\).</p>

<p>Finally if \(c &lt; 1\) then \(\frac{1}{c} &gt; 1\) and by the reciprocal rule
\(\frac{1}{c^\frac{1}{n}} \rightarrow 1\).</p>

<p>Did you get that? Me neither.</p>

<h3 id="divergent-sequences">Divergent Sequences</h3>

<p>A sequence \(a_n\) is said to <strong>diverge to
infinity</strong> if
\(\forall K \in \mathbb{R}\; \exists N : n &gt; N \implies a_n &gt; K\).</p>

<p>In plain English, there is a point in \(a_n\) where the terms are greater
than any real number one picks.</p>

<p>We denote this as \(a_n \rightarrow \infty\). \(a_n\) diverges to \(-\infty\)
if \(-a_n \rightarrow \infty\).</p>

<p>A divergent sequence that doesn't go off to infinity is said to
<strong>oscillate</strong>.</p>

<h3 id="basic-convergent-sequences">Basic convergent sequences</h3>

\[\begin{align}  &amp;  \lim_{n
\rightarrow \infty} \frac{1}{n^p} = 0  &amp;  \forall p &gt; 0 \\  &amp; 
\lim_{n \rightarrow \infty} c^n = 0  &amp;  \forall c : \|c\| &lt; 1 \\
 &amp;  \lim_{n \rightarrow \infty} c^\frac{1}{n} = 1  &amp;  \forall c &gt; 0
\\  &amp;  \lim_{n \rightarrow \infty} n^p c^n = 0  &amp;  \forall p &gt; 0
\land \|c\| &lt; 1 \\  &amp;  \lim_{n \rightarrow \infty}
\frac{c^n}{n!} = 0  &amp;  \forall c \in \mathbb{R} \\  &amp;  \lim_{n
\rightarrow \infty} (1 + \frac{c}{n})^n = e^c  &amp;  \forall c \in
\mathbb{R} \end{align}\]

<h2 id="series">Series</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part3/note17.pdf">Link to the PDF.</a></p>

<p>A series \(\sum a_n\) is a pair of sequences:</p>

<ol>
  <li>A sequence \(a_n\) called the <strong>sequence of terms</strong></li>
  <li>A sequence \(s_n\) called the <strong>sequence of partial sums</strong> defined as
\(s_n = \sum_{k=0}^{n} a_k.\)</li>
</ol>

<p>If the sequence of partial sums converges to \(s\), then the series
converges to the sum \(s\); \(\sum_{n=0}^{\infty} a_n = s.\) Divergent if
not.</p>

<p>A standard one is the geometric series, \(\sum r^n\) which will always
converge to \(\frac{1}{1-r}\) when \(|r| &lt; 1\). You can prove this by
working out \(rs_n - s_n\).</p>

\[\begin{align} s_n - rs_n = 1 - r^{n+1}
 &amp; \Longleftrightarrow s_n(1-r) = 1 - r^{n+1}\\  &amp; \Longleftrightarrow
s_n = \frac{1 - r^{n+1}}{1-r}. \end{align}\]

<p>And \(|r| &lt; 1\) means that
\(r\)-power will converge to 0.</p>

<p>A standard divergent series is the harmonic series, \(\sum \frac{1}{n}\).
You can prove this by estimating partial sums
\(s_2, s_4, s_8, s_{16}, ...\) and see that it will forever build to
\(1 + 0.5 + 0.5 + 0.5 + ...\). \(s_{2^n} &gt; 1 + \frac{n}{2}.\) Though any
series \(\sum \frac{1}{n^k}\) where \(k &gt; 1\) will always converge.</p>

<h3 id="basic-properties-of-convergent-series">Basic Properties of Convergent Series</h3>

<ol>
  <li><strong>Sum rule</strong>, \(\sum a_n\) converges to \(s,\; \sum b_n\) converges to
\(t \implies \sum (a_n + b_n)\) converges to \(s + t\).</li>
  <li><strong>Multiple rule</strong>, \(\sum a_n\) converges to
\(s, \; \lambda \in \mathbb{R} \implies \sum \lambda a_n\) converges
to \(\lambda s\).</li>
  <li>\(\sum a_n\) converges \(\implies a_n \rightarrow 0\).</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$\sum</td>
          <td>a_n</td>
          <td>\(converges\)\implies \sum a_n$$ also converges.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<p>No. 4 is only useful really when you have an "oscillating" series,
like \(\sum \frac{(-1)^n}{2^n}\).</p>

<h3 id="the-comparison-test">The Comparison Test</h3>

<p>Suppose that \(0 \leq a_n \leq b_n\) for all
\(n\),</p>

<ol>
  <li>If \(\sum b_n\) converges then so does \(\sum a_n\)</li>
  <li>If \(\sum a_n\) diverges then so does \(\sum b_n\).</li>
</ol>

<p>Usually with comparison test questions you just pull out equations from
thin air, but (especially with rational functions) there's usually a
technique for doing so based on the largest power of \(n\), and slowly
chipping away a piece at a time until we get a very simple expression.</p>

<h4 id="exercises">Exercises</h4>

<p>Determine whether the following series converges or
diverges:</p>

<ol>
  <li>
\[\sum \frac{n+2}{n^3 - n^2 + 1}\]
  </li>
  <li>
\[\sum \frac{n^2 + 4}{2n^3 - n + 1}\]
  </li>
</ol>

<p><strong><em>Solution 1.</em></strong> Take a look at the fraction \(\frac{n+2}{n^3 - n^2 + 1}\).
If we want to find a smaller fraction, we want a smaller numerator and a
larger denominator than our current fraction.</p>

\[\begin{align}  &amp;  n+2 \geq
n  &amp;  n^3 - n^2 + 1 \geq n^3 \end{align}\]

<p>So we get the fraction
\(\frac{n}{n^3}\) or \(\frac{1}{n^2}\) which we know will converge as a
series. No help there.</p>

<p>For a larger fraction, we want a bigger numerator and a smaller
denominator;</p>

\[\begin{align} n+2  &amp; \leq n + 2n \textrm{ or } 3n \\n^3 - n^2 + 1 \geq n^3 - n^2 \textrm{ or } n^2 (n -1)  &amp; \geq \frac{n^3}{2}. \end{align}\]

<p>Admittedly, at the end we were pulling
expressions out of thin air a bit but hopefully you can see that it's
all valid. Thus \(\frac{n+2}{n^3 - n^2 + 1} \leq \frac{6}{n^2}\) and since
we know \(\sum \frac{6}{n^2}\) converges (\(6 \sum \frac{1}{n^2}\)) the
original one must too.</p>

<p><strong><em>Solution 2.</em></strong> We're gonna be cheat-y and say that this sequence does
diverge. To show this, let's find a smaller fraction which diverges.</p>

\[\begin{align} n^2 + 4  &amp; \geq n^2 \\ 2n^3 -n + 1  &amp; \leq 2n^3
\end{align}\]

<p>(the \(-n\) more than cancels out the \(+1\)) and we can see
that this gives us \(\frac{1}{2n} \leq \frac{n^2 + 4}{2n^3 -n+1}\) And oh
no, \(\sum \frac{1}{2n}\) diverges (multiple rule) so the original one
must too.</p>

<h3 id="the-ratio-test">The Ratio Test</h3>

<table>
  <tbody>
    <tr>
      <td>If $$</td>
      <td>\frac{a_{n+1}}{a_n}</td>
      <td>\rightarrow L$$ then</td>
    </tr>
  </tbody>
</table>

<ol>
  <li>\(0 \leq L &lt; 1 \implies \sum a_n\) converges.</li>
  <li>\(L &gt; 1\) or \(L \textrm{ is } \infty \implies \sum a_n\) diverges.</li>
  <li>\(L = 1 \implies\) the test is inconclusive.</li>
</ol>

<p>Useful in dealing with factorials.</p>

<h3 id="basic-convergent-series">Basic Convergent Series</h3>

<ul>
  <li>\(\sum_{n=0}^{\infty} r^n = \frac{1}{1-r}\) for all \(r : \|r\| &lt; 1\)</li>
  <li>\(\sum \frac{1}{n^k}\) converges for all \(k &gt; 1\)</li>
  <li>\(\sum n^k r^n\) converges for \(k &gt; 0 \land \|r\| &lt; 1\)</li>
  <li>\(\sum_{n=0}^{\infty} \frac{c^n}{n!} = e^c\) for all
\(c \in \mathbb{R}\)</li>
</ul>

<h3 id="basic-divergent-series">Basic Divergent Series</h3>

<ul>
  <li>\(\sum \frac{1}{n^k}\) diverges for all \(k &lt; 1\).</li>
</ul>

<h3 id="power-series">Power series</h3>

<p>A <strong>power series</strong> is one of the form \(\sum a_n x^n\). Usually we start
at \(n = 0\), such that the sequence goes \(a_0, a_1x, a_2x^2, ...\)</p>

<h4 id="basic-properties-of-power-series">Basic Properties of Power Series</h4>

<p>Let</p>

\[\begin{align} f(x)  &amp; =
\sum_{n=0}^{\infty} a_nx^n  &amp;  x \in (-R_1, R_1) \\ g(x)  &amp; =
\sum_{n=0}^{\infty} b_nx^n  &amp;  x \in (-R_2, R_2) \end{align}\]

<p>For
positive \(R_1, R_2\), and let \(R = \min(R_1, R_2)\). Then for all
\(x \in (-R , R)\),</p>

<ol>
  <li>If \(f(x) = g(x)\) then \(a_n = b_n\) for all \(n\)</li>
  <li><strong>Sum rule,</strong> \(f(x) + g(x) = \sum_{n=0}^{\infty} (a_n + b_n) x^n\)</li>
  <li><strong>Multiple rule,</strong>
\(\lambda f(x) = \sum_{n=0}^{\infty} \lambda a_nx^n \; \forall \lambda \in \mathbb{R}\)</li>
  <li><strong>Product rule,</strong>
\(f(x)g(x) = \sum_{n=0}^{\infty} (a_0b_n + a_1b_{n-1} + ... + a_{n-1}b_1 + a_nb_0)x^n\).</li>
</ol>

<p>This also gives the general binomial theorem, which for any
\(q \in \mathbb{Q}, x \in (-1, 1)\)</p>

\[\begin{align} (1+x)^q  &amp; =
\sum_{n=0}^{\infty} {q \choose n} x^n \\ {q \choose n}  &amp; =
\frac{q!}{n!(q-n)!} \end{align}\]

<h3 id="radius-of-convergence">Radius of convergence</h3>

<p><strong><em>Lemma.</em></strong> If \(\sum a_n R^n\) converges for some \(R \geq 0\), then
\(\sum a_n x^n\) converges \(\forall x : \|x\| &lt; R\). <em>(proof in notes)</em></p>

<p>\(R \geq 0\) is the <strong>radius of convergence</strong> of a power series
\(\sum a_n x^n\) if it converges according to the above, and diverges if
\(|x| &gt; R\). If a series converges \(\forall x\) then the radius is
infinity.</p>

<p>If the series \(\sum a_n x^n\) has a conv. rad \(R\) then it defines a
function</p>

\[\begin{align}  &amp; f(x) = \sum_{n=0}^{\infty} a_nx^n
 &amp; \forall x \in (-R, R) \end{align}\]

<p>You can find the <a href="https://www.youtube.com/watch?v=4L9dSZN5Nvg">radius of convergence using ratio
test</a>, essentially
evaluate \(\lim_{n \rightarrow \infty} \|\frac{a_{n+1}}{a_n}\| &lt; 1.\) and
you will get an \(\|x\| &lt; ...\) where that something is your radius.</p>

<h3 id="addendum-partial-fractions">Addendum: Partial Fractions</h3>

<p>Partial fractions can be important in series.</p>

<p>Where a rational function is decomposed into a sum of simpler fractions,
such as \(\frac{cx+d}{(x-a)(x-b)} = \frac{A}{x-a} + \frac{B}{x-b}.\)</p>

<p>When there is no repeated factor, simply substituting \(x = a, x=b\) can
eliminate a term and make it easy to find the unknowns \(A, B\). When
there is however, we need to just pick values of \(x\) and solve from
there.</p>

<p>If there is a repeated factor \((x-a)^n\), the partial sum will have all
the fractions with denominations \((x-a), (x-a)^2,...\) up to \((x-a)^n\)</p>

<p>If the degree of the numerator is higher than the denominator, then we
have to divide out the numerator (polynomial long division) to get a
valid separable fraction,
\(\frac{x^3 + 3x}{(x+1)(x-3)} = x+2 + \frac{10x+6}{(x+1)(x-3)}.\)</p>

<h2 id="recurrences">Recurrences</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part3/note16.pdf">Link to the PDF.</a></p>

<p><em>Like the differential equations of sequences.</em></p>

<p>A recurrence is a sequence defined, funnily enough, recursively.</p>

<p>For example, the Fibonacci sequence \(F_n\) is defined as
\(F_n = F_{n-1} + F_{n-2}\) \(F_0 = 0, F_1 = 1.\)</p>

<p>The <em>Towers of Hanoi</em> game's number of steps taken was computed to be
the recurrence \(T_n = 2T_{n-1} + 1.\) For \(T_0 = 0\) this gives
\(0, 1, 3, 7, 15, ...\) which we can see is \(T_n = 2^n - 1\). This latter
form is the <strong>closed form</strong> of \(T_n\) and allows for immediate
computation.</p>

<p>We are looking at solving linear recurrences with constant coefficients,
of the form \(x_n + a_1x_{n-1} + ... a_kx_{n-k} = f(n)\) where \(f\) is a
given function. (If the terms from 1 to k are given, then this describes
a unique sequence.)</p>

<h3 id="homogenous-recurrences">Homogenous recurrences</h3>

<p>More importantly, we will look at <strong>homogenous</strong> (\(f(n) = 0\))
recurrences with \(k = 2\). Thus, we want the general solution of
\(x_n + ax_{n-1} + bx_{n-2} =0.\)</p>

<p>In the general case, we want to look for solutions of the form
\(A\lambda^n\) where \(\lambda, A\) are constants. A bit of deriving later,
we can get the auxiliary equation in lambda.</p>

<p>The <strong>auxiliary equation</strong> of the recurrence
\(x_n + ax_{n-1} + bx_{n-2} = 0\) is \(\lambda^2 + a\lambda + b = 0.\)</p>

<p><strong><em>General Solution.</em></strong> of the recurrence
\(x_n + ax_{n-1} + bx_{n-2} = 0\),</p>

<p>Let \(\lambda_1, \lambda_2\) be the roots of the  auxiliary equation.</p>

<ul>
  <li>
\[\lambda_1 \neq \lambda 2 \implies x_n = A\lambda_1^n + B\lambda_2^n\]
  </li>
  <li>
\[\lambda_1 = \lambda_2 \implies x_n = A\lambda_1^n + Bn\lambda_2^n\]
  </li>
</ul>

<p>For constants \(A, B\), which can be found if the first two terms of the
sequence are known.</p>

<h3 id="exercise-2">Exercise</h3>

<p>Find a closed form for the Fibonacci sequence.</p>

<p><strong><em>Solution.</em></strong> We have \(F_n = F_{n-1} + F_{n-2}\) so
\(F_n - F_{n-1} - F_{n-2} = 0\). The auxiliary equation is
\(\lambda^2 - \lambda - 1 = 0\) Which has roots
\(\lambda = \frac{1 \pm \sqrt{5}}{2}\) Or \(\phi\) and \(-(\frac{1}{\phi})\)
(the golden ratio phi). Hence
\(F_n = A(\frac{1 + \sqrt{5}}{2}) + B(\frac{1 - \sqrt{5}}{2}).\) We know
\(F_0 = 0, F_1 = 1\) so if we substitute these values in, we get that
\(A = \frac{1}{\sqrt{5}}, B = -\frac{1}{\sqrt{5}}\).</p>

\[\begin{align} F_n
 &amp; = \frac{1}{\sqrt{5}}(\frac{1 + \sqrt{5}}{2})
-\frac{1}{\sqrt{5}}(\frac{1 - \sqrt{5}}{2}). \\  &amp; =
\frac{1}{\sqrt{5}}(\phi^n + \phi^{-n}). \end{align}\]

<p>Actually by some magic we can further simplify down to
\(F_n = \left \lfloor{\frac{\phi^n}{\sqrt{5}}}\right \rfloor\)</p>

<p>Non-homogenous recurrences have the \(f(n)\) bit not 0.</p>

<h3 id="non-homogenous-recurrences">Non-homogenous recurrences</h3>

<p>Finding solutions to non-homogenous recurrences of the form
\(x_n + ax_{n-1} + bx_{n-2} = f(n)\)</p>

<ol>
  <li>Find the general solution \(x_n = h_n\) of the homogenous recurrence:
\(x_n + ax_{n-1} + bx_{n-2} = 0.\) (will have 2 arbitrary constants)</li>
  <li>Find <em>any</em> particular solution \(x_n = p_n\) of the original
recurrence.</li>
  <li>The general solution will be \(x_n = h_n + p_n\).</li>
</ol>

<p>Finding the particular solution \(p_n\) is not straight forward, but the
technique is to try solutions that are <em>similar in form</em> to \(f(n)\).
Substitute these into the original recurrence (as they are solutions!)
and try solve.</p>

<p>If \(f(n)\) is a constant, try find a constant \(k\).</p>

<p>If \(f(n)\) is a polynomial, try find a <em>same degree</em> polynomial.
(substitute in a general polynomial)</p>

<p>Etc. just like differential equations.</p>

<h2 id="decimal-representation-of-reals">Decimal Representation of Reals</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part3/note16.pdf">Link to the PDF.</a></p>

<p>Rational numbers can be represented by expansions, so can repeating
decimals.</p>

<p>Repeating decimals can be expressed as a fraction by summing an
appropriate geometric series.</p>

<p>For example, \(0.4\overline{9}\) can be expanded as follows;</p>

\[\begin{align} 0.4\overline{9}  &amp; = \frac{4}{10} + \frac{0}{10^2} +
\frac{9}{10^3} + ... \\  &amp; = \frac{4}{10} + \frac{9}{10}(1 +
\frac{1}{10} + \frac{1}{10^2} + ...) \\  &amp; = \frac{4}{10} +
\frac{9}{100}(\frac{1}{1-0.1}) = \frac{4}{10} + \frac{9}{90} =
\frac{4}{10} + \frac{1}{10} = \frac{1}{2}. \end{align}\]

<p>The decimal expansion of any arbitrary real can be computed by
squeezing between two converging sequences.</p>

<p>Expand if you really want to see...</p>

<p>If that real \(x\) is not an integer, it is between one integer and the
next consecutive one. Now divide the integer gap into 10 fractions - if
\(x\) is not on a fraction then it is between two consecutive division
points, \(a_0 + \frac{a_1}{10} &lt; x &lt; a_0 + \frac{a_1 + 1}{10}\) Where
\(a_1 \in \{0..9\}\). We can then repeat to get
\(a_0 + \frac{a_1}{10} + \frac{a_2}{10^2} &lt; x &lt; a_0 + \frac{a_1}{10} + \frac{a_2 + 1}{10^2}\)
etc etc.</p>

<p>Every rational has a terminating or repeating decimal, there's some
formal <em>rigorous</em> waffle and then you compute it via long division
really.</p>

<p>Not sure why this is here save the fact that yes decimals are related to
series.</p>


            <br/>
            <hr style="
                padding: 5px;
                border-radius: 1em;
                background-color: whitesmoke;
            ">
            <br/><h1 id="calculus">Calculus</h1>
       
            <h2 id="limits-and-continuity">Limits and Continuity</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note19.pdf">Link to the PDF.</a></p>

<p><em>(Definitions)</em> let a function \(f: I \longrightarrow \mathbb{R}\) be
defined on an interval \(I\). We're interested in a point \(a\) in that
interval, where \(f\) may or may not be defined.</p>

<p>When \(f(x)\) tends to a value \(l\) as \(x\) tends towards \(a\) from the left
(negative side), we write the limit \(\lim_{x \rightarrow a-} f(x) = l\)
Conversely, if \(f(x)\) tends to \(l\) as \(x\) to \(a\) from the other side we
write \(\lim_{x \rightarrow a+} f(x) = l\) If
\(\lim_{x \rightarrow a-} f(x) = l\) and
\(\lim_{x \rightarrow a+} f(x) = l\) then
\(\lim_{x \rightarrow a} f(x) = l.\) What this says that if every sequence
\(x_n\) in \(I\) which converges to \(a\), \(x_n \neq a\), then \(\forall n\) the
sequence \(f(x_n)\) converges to \(l\).</p>

<h3 id="floor-and-ceiling-functions">Floor and Ceiling functions</h3>

<p>The \(\left \lfloor{\textrm{floor}}\right \rfloor\) and
\(\left \lceil{\textrm{ceiling}}\right \rceil\) functions always round
down and up respectively to the nearest integer.</p>

<p>\(\forall k \in \mathbb{Z}\):</p>

\[\begin{align}  &amp;  \lim_{x \rightarrow k-}
\left \lfloor{x}\right \rfloor = k-1,  &amp;  \lim_{x \rightarrow k+}
\left \lfloor{x}\right \rfloor = k; \end{align}\]

\[\begin{align}  &amp; 
\lim_{x \rightarrow k-} \left \lceil{x}\right \rceil = k,  &amp; 
\lim_{x \rightarrow k+} \left \lceil{x}\right \rceil = k+1.
\end{align}\]

<h4 id="exercise">Exercise</h4>

<p>Prove that \(\lim_x \rightarrow 0 \frac{1}{x}\) does not
exist.</p>

<p><strong><em>Proof.</em></strong> We need to find a sequence \(x_n \rightarrow 0\) where
\(f(x_n)\) does not converge. We can magic the sequence
\(x_n = \frac{1}{n}\) which does converge to 0, but \(f(x_n) = n\) is an
unbounded sequence which does not converge. Hence, the limit does not
exist.</p>

<h3 id="combination-rules-for-limits">Combination rules for limits</h3>

<p>If \(\lim_{x \rightarrow a} f(x) = l\)</p>

<p>and \(\lim_{x \rightarrow a} g(x) = m\) then</p>

<ul>
  <li><strong>Sum</strong> rule: \(\lim_{x \rightarrow a} (f(x) + g(x)) = l + m\)</li>
  <li><strong>Multiple</strong> rule:
\(\lim_{x \rightarrow a} \lambda f(x) = \lambda l\),
\(\lambda \in \mathbb{R}\)</li>
  <li><strong>Product</strong> rule: \(\lim_{x \rightarrow a} f(x)g(x) = lm\)</li>
  <li><strong>Quotient</strong> rule:
\(\lim_{x \rightarrow a} \frac{f(x)}{g(x)} = \frac{l}{m}\) provided
\(m \neq 0\)</li>
</ul>

<h3 id="squeeze-rule-for-limits">Squeeze rule for limits</h3>

<p>If \(f(x) \leq g(x) \leq h(x)\) for all
\(x \neq a\),</p>

\[\lim_{x \rightarrow a} f(x) = l \land \lim_{x \rightarrow a} h(x) = l \implies \lim_{x \rightarrow a} g(x) = l.\]

<h3 id="continuous-functions">Continuous functions</h3>

<p>A continuous function is one with no "jumps" -</p>

<p>A function \(f : D \longrightarrow \mathbb{R}\) (\(D \subseteq \mathbb{R}\))
is <strong>continuous at a point \(a\)</strong> if \(\lim{x \rightarrow a} f(x)\) exists
and equals \(f(a)\). \(f\) is continuous if it is continuous at every point in
the interval.</p>

<h4 id="combination-rules-for-continuous-functions">Combination rules for continuous functions</h4>

<p>If \(f, g\) are
continuous at \(a\), then so are</p>

<ul>
  <li>The sum \(f+g\)</li>
  <li>The scalar multiple \(\lambda f\) (\(\lambda \in \mathbb{R}\))</li>
  <li>The product \(fg\)</li>
  <li>The quotient \(\frac{f}{g}\) provided \(g(a) \neq 0\)</li>
</ul>

<p>If \(f\) is continuous at \(a\) and \(g\) is continuous at \(f(a)\), then the
composite \(g \circ f\) is also continuous at \(a\).</p>

<p><em>\(g \circ f \equiv g(f(x))\).</em></p>

<h4 id="basic-continuous-functions">Basic continuous functions</h4>

<ul>
  <li>polynomials and rational functions*</li>
  <li>The modulus/absolute function</li>
  <li>The square root function, and nth root function where
\(n \in \mathbb{Z}^+\)</li>
  <li>Trig functions</li>
  <li>Exponential functions</li>
  <li>Functions defined by power series</li>
</ul>

<p>*The domain of rational functions exclude the divide by zero bit and so
is continuous.</p>

<h3 id="intermediate-value-theorem">Intermediate Value Theorem</h3>

<p>If
\(f : [a, b] \longrightarrow \mathbb{R}\) is continuous, \(f(a), f(b)\) have
opposite signs, then there is a point \(c\) between \(: f(c) = 0\).</p>

<h4 id="exercise-1">Exercise</h4>

<p>Show that there is a number
\(x : x^{179} + \frac{163}{1 + x^2} = 119\).</p>

<p><strong><em>Proof.</em></strong> Rearrange, let
\(f(x) = x^{179} + \frac{163}{1 + x^2} - 119\).</p>

<p>\(f(0) &gt; 0,\; f(1) &lt; 0\) thus by intermediate value theorem
\(\exists x \in [1, 0] : f(x) = 0\).</p>

<p><em>(You do have to conjure numbers to do this question)</em></p>

<h3 id="extreme-value-theorem">Extreme Value Theorem</h3>

<p>If \(f : [a, b] \longrightarrow \mathbb{R}\)
is continuous, then \(\exists m, M \in [a, b]:\)</p>

\[\begin{align}  &amp; f(m)
\leq f(x) \leq f(M)  &amp; \forall x \in [a, b] \end{align}\]

<p>Basically, there's a maximum and minimum value of a function in a
bound.</p>

<h2 id="differentiation">Differentiation</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note20.pdf">Link to the PDF.</a></p>

<p>If the limit of a real function \(f(x)\) at point \(a\)
\(lim_{h \rightarrow 0} \frac{f(a + h) - f(a)}{h}\) exists, then \(f(x)\) is
differentiable at \(a\), we get \(f'(a)\).</p>

<p>The \(\frac{d}{dx}\) thing is called Leibniz notation.</p>

<p><strong><em>Theorem.</em></strong> If \(f\) is differentiable at \(a\), \(f\) is also continuous
at \(a\). However, \(f\) may be continuous but <em>not</em> differentiable.</p>

<p>For example, the function \(f(x) = |x|\) is continuous at 0 but not
differentiable, because at 0 the relevant limit does not exist (there's
a crinkle).</p>

<p>Also though the notes is very from principles <em>""RIGOUR""</em>
\(\frac{d(x^n)}{dx} = nx^{n-1}\) - in case you didn't know.</p>

<h3 id="combination-rules-for-derivatives">Combination Rules for Derivatives</h3>

<p>If \(f, g\) are differentiable, the
following are also differentiable and follow these rules:</p>

<ul>
  <li><strong>Sum</strong> rule (\(f + g\)): \((f +g)' = f' + g'\)</li>
  <li><strong>Multiple</strong> rule (\(lambda f:\lambda \in \mathbb{R}\)):
\((\lambda f)' = \lambda f'\)</li>
  <li><strong>Product</strong> rule (\(fg\)): \((fg)' = fg' + gf'\)</li>
  <li><strong>Quotient</strong> rule (\(f/g\)):
    \((\frac{f}{g})' = \frac{gf' - fg'}{g^2}\)</li>
</ul>

<h3 id="trig-derivatives">Trig derivatives</h3>

\[\begin{array}{ccc}  &amp;  \frac{d(\sin x)}{dx} = \cos x  &amp;  \frac{d(\cos
x)}{dx} = -\sin x  &amp; \frac{d(\tan x)}{dx} = \frac{1}{\cos ^2 x} =
\sec ^2 x. \end{array}\]

<h3 id="the-chain-rule">The Chain Rule</h3>

<p>Providing the functions \(y = f(z), \; z = g(x)\) are differentiable</p>

\[\begin{align} \frac{dy}{dx}  &amp; = \frac{dy}{dz} \times \frac{dz}{dx}
\\ \textrm{or } (f \circ g)'(x)  &amp; = g'(x) f'(g(x)) \end{align}\]

<h3 id="differentiation-of-functions-def-by-power-series">Differentiation of functions def' by power series</h3>

<p>If \(\sum(a_nx^n\) is a
power series with radius of convergence R, and \(f\) is defined
\(f(x) = \sum_{n=0}^\infty a_nx^n : x \in (-R, R)\) then it is differentiable
and \(f'(x) = \sum_{n=1}^\infty na_nx^{n-1}\)</p>

<p><em>Pay attention: \(n=0\) changes to \(n=1\)</em></p>

<p>The exponential function \(\exp(x)\) or \(e^x\) can be defined as a power
series (its Maclaurin series) and thus differentiable to get... \(e^x\).</p>

<h4 id="exercise-2">Exercise</h4>

<p>Find \(\sum_{n=1}^\infty \frac{n}{2^n}\).</p>

<p><strong><em>Solution.</em></strong> You've got to bear with me on this one, because the given
solution is clearly written by someone who knew the answer already and
have left out a lot of the logic leaps.</p>

<p>We start with a standard definition, for \(|x| = 1\)
\(\sum_{n=0}^\infty x^n = \frac{1}{1-x}\) We differentiate the left to get
\(\sum_{n=1}^\infty nx^{n-1} = \frac{d}{dx} (\frac{1}{1-x}).\) And the
right resolves into \((1-x)^{-2}\).</p>

<p>This derivative form of the power set is very similar to the sum we want
to find, so much so that if we take \(x = \frac{1}{2}\),
\(\sum_{n=1}^\infty n(\frac{1}{2})^{n-1} = \sum_{n=1}^\infty \frac{n}{2^{n-1}}\)
Which resolves down into \(4\) since we can just sub in \(x\) to the other
side. Note that if we want to get to \(\frac{n}{2^n}\) from
\(\frac{n}{2^n-1}\) we have to multiply by \(\frac{1}{2}\), so we multiply
both sides by a half to get \(\sum_{n=1}^\infty \frac{n}{2^n} = 2.\)</p>

<h2 id="properties-of-differentiable-functions">Properties of Differentiable Functions</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note21.pdf">Link to the PDF.</a></p>

<p><em>(Things to be aware of)</em> Turning points, of which are local <em>maxima</em>
and <em>minima</em>.</p>

<h3 id="turning-point-theorem">Turning Point Theorem</h3>

<p>If a differentiable function \(f\) has a turning
point at \(a\), \(f'(a) = 0\).</p>

<p>All points \(a\) where \(f(a) = 0\) are called <strong>stationary points</strong>, which
aren't necessarily turning points. A stationary point which is neither
a local maximum nor minimum is a <strong>point of inflection</strong></p>

<p>To locate maxima and minima of a continuous function in a range \([a, b]\), we
need only consider values at</p>

<ul>
  <li>The stationary points of \(f\) in \([a, b]\)</li>
  <li>The end points \(a, b\)</li>
</ul>

<h3 id="rolles-theorem">Rolle's Theorem</h3>

<p>If \(f : [a, b] \longrightarrow \mathbb{R}\) is
continuous, is differentiable on \(a, b\) and \(f(a) = f(b)\) then there is a
point \(c \in a, b : f'(c) = 0\)</p>

<p>You have two end points with the same \(y\) value, so in the middle there
must be <em>some</em> value which is a stationary point. (on horizontal lines,
all the points work.)</p>

<h3 id="mean-value-theorem">Mean Value Theorem</h3>

<p>If \(f\) is continuous and differentiable over
\([a, b]\) then \(\exists c \in [a,b] :\) \(f'(c) = \frac{f(b) - f(a)}{b-a}.\)</p>

<p>Which is just Rolle's theorem but on a angled graph.</p>

<h4 id="consequences-of-the-mvt">Consequences of the MVT</h4>

<p>Suppose \(f\) is continuous on \([a,b]\) and
differentiable on \((a,b)\),</p>

<ol>
  <li>
    <ul>
      <li>\(f'(x) = 0 \forall x \in (a, b) \implies f\) is constant on
\([a, b]\)</li>
      <li>\(f'(x) &gt; 0 \forall x \in (a, b) \implies f\) is <em>strictly</em>
increasing</li>
      <li>\(f'(x) &lt; 0 \forall x \in (a, b) \implies f\) is <em>strictly</em>
decreasing</li>
    </ul>
  </li>
  <li><strong>Second Derivative Test.</strong> Suppose \(f'(c) = 0\).
\(f''(c) &gt; 0 \implies f\) has local <em>mini</em>mum; \(f''(c) &lt; 0 \implies f\)
has local <em>maxi</em>mum.</li>
</ol>

<p>For curve sketching:</p>

<ul>
  <li>Find stationary points, their co-ordinates, and their nature
(max/min/inflection)</li>
  <li>Find where \(f(x) = 0\), i.e. intersects \(x\) axis</li>
  <li>Find \(f\) where \(x = 0\), i.e. y-intercept</li>
  <li>Determine what happens to \(f(x)\) as \(x \rightarrow \pm\infty\)
(asymptotes?)</li>
  <li>Investigate near where (if) \(f(x) \rightarrow \infty\), where a
divide by zero is found or some other asymptote.</li>
</ul>

<h2 id="lhopitals-rule-and-implicit-differentiation">L'Hopital's Rule and Implicit Differentiation</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note22.pdf">Link to the
PDF.</a></p>

<h3 id="lhopitals-rule">L'Hopital's Rule</h3>

<p>Suppose that \(f(a) = 0, g(a) = 0\). If</p>

<p>\(\lim_{x \rightarrow a} \frac{f'(x)}{g'(x)}\) exists then so does
\(\lim_{x \rightarrow a} \frac{f(x)}{g(X)}\) also exists,
\(\lim_{x\rightarrow a} \frac{f(x)}{g(x)} = \lim_{x \rightarrow a} \frac{f'(x)}{g'(x)}.\)</p>

<p>You can repeat this and keep on differentiating if you keep getting
\(\frac{0}{0}\), however there is no guarantee that you will get anywhere.</p>

<h3 id="implicit-differentiation">Implicit differentiation</h3>

<p>Implicit differentiation deals with functions not of the form
\(y = f(x)\), such as \(x^2 + y^2 = 1\).</p>

<p>Two methods (that work the same):</p>

<ul>
  <li>Either consider a \(y\) term as a function of \(x\), thus because of
chain rule append a \(\frac{dy}{dx}\) to the end of each \(y\) term,
differentiate \(x\) terms as normal.</li>
</ul>

\[\begin{align} \therefore
    d(x^2 + y^2 = 0)/dx  &amp; \implies 2x + 2y\frac{dy}{dx} = 0 \\
     &amp; \implies \frac{dy}{dx} = -\frac{2x}{2y} = -\frac{x}{y}.
    \end{align}\]

<ul>
  <li>Or, consider both as functions and append a \(dx\) or \(dy\).</li>
</ul>

\[\begin{align} \therefore d(x^2 + y^2 = 0)/dx  &amp; \implies 2xdx +
    2ydy = 0\\  &amp; \implies 2ydy = -2xdx\\  &amp; \implies \frac{dy}{dx} =
    -\frac{2x}{2y} = -\frac{x}{y}. \end{align}\]

<h4 id="exercise-3">Exercise</h4>

<p>Find \(\frac{dy}{dx}\) of \(y \sin x + \tan^{-1} y = 0\) in
terms of \(y, x\).</p>

<p><strong><em>Solution.</em></strong> First, differentiate \(y \sin x\). By product rule,</p>

<p>\(d(y \sin x) = \sin(x)dy + y\cos(x)dx\) Then \(\tan^{-1} y\) is a standard
derivative, \(d(\tan^{-1} y) = \frac{1}{1+y^2} dy\) Thus we get
\(\sin(x)dy + y\cos(x)dx + \frac{1}{1+y^2}dy = 0\) Collecting terms,
\((\sin(x) + \frac{1}{1+y^2})dy = (-y\cos(x))dx\) Dividing through,
\(\frac{dy}{dx} = \frac{-y \cos x}{\sin x + \frac{1}{1+ y^2}}\) Which one
can simplify if they wish.</p>

<h2 id="differentiating-inverse-functions">Differentiating Inverse Functions</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note23.pdf">Link to the PDF.</a></p>

<p>Recall from 130 <a href="../cs130/index.html#func-2">Injection, Bijection, and
Surjection</a>. Recall also the range of
a function \(f : A \longrightarrow B\) is
\(\{y \in B : f(x) = y, x \in A \}\).</p>

<p>Let
\(f : [a, b] \longrightarrow \mathbb{R}\) be a continuous function. If \(f\)
differentiable on \((a, b)\) and
\(f'(x) &gt; 0 \lor f'(x) &lt; 0 \; \forall x \in (a, b)\), then \(f\) has an
inverse function \(f^{-1}\) which is differentiable. Let \(y = f(x);\)</p>

\[\begin{align}  &amp;  (f^{-1})'(y) = \frac{1}{f'(x)} \equiv  &amp; 
\frac{dx}{dy} = \frac{1}{\frac{dy}{dx}} \end{align}\]

<h3 id="example">Example</h3>

<p>To find the derivative of \(f\) given
\(f(x) = x^\frac{1}{n}\) for some fixed \(n &gt; 0\). The inverse function is
gotten by writing \(y = x^\frac{1}{n}\), such that \(x = y^n\), thus
\(f^{-1}(x) = y^n\). So</p>

\[\begin{align}  &amp; \frac{dx}{dy} = ny^{n-1}  &amp;  \frac{dy}{dx} =
\frac{1}{ny^{n-1}}. \end{align}\]

<p>\(y = x^\frac{1}{n}\) so \(\frac{dy}{dx} = y^{n-1} = x^\frac{n-1}{n}\), thus</p>

\[\begin{align} f\'(x)  &amp; = \frac{dy}{dx} = \frac{1}{nx^\frac{n-1}{n}}
\\  &amp; = \frac{1}{n} x^\frac{1}{n-1}. \end{align}\]

<p>We can differentiate any rational power of \(x\) by the following:
\(\frac{d}{dx}x^\frac{p}{q} = ... = \frac{p}{q} x^{\frac{p}{q}-1}.\) same
way for regular differentiation.</p>

<h3 id="trigonometric-inverse-derivatives">Trigonometric Inverse Derivatives</h3>

<ul>
  <li>\(\mathbf{sin}^{-1}\). The inverse for the function
\(\sin : [\frac{-\pi}{2}, \frac{\pi}{2}] \longrightarrow [-1, 1]\);</li>
</ul>

\[\begin{align}  &amp; \frac{d}{dx}\sin^{-1}(x) = \frac{1}{\sqrt{1 -
    x^2}}  &amp; (-1&lt;x&lt;1) \end{align}\]

<ul>
  <li>\(\mathbf{cos}^{-1}\). The inverse for the function
\(\cos : [0, \pi] \longrightarrow [-1, 1]\);</li>
</ul>

\[\begin{align}
     &amp; \frac{d}{dx}\cos^{-1}(x) = \frac{-1}{\sqrt{1-x^2}}  &amp; (-1 &lt; x
    &lt; 1) \end{align}\]

<ul>
  <li>\(\mathbf{tan}^{-1}\). The inverse for the function
\((\frac{-\pi}{2}, \frac{\pi}{2}) \longrightarrow \mathbb{R}\);</li>
</ul>

\[\begin{align}  &amp; \frac{d}{dx}\tan^{-1}(x) = \frac{1}{1+x^2}  &amp;  (x
    \in \mathbb{R}) \end{align}\]

<h2 id="integration">Integration</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note24.pdf">Link to the
PDF.</a></p>

<h3 id="definition-of-integration">Definition of integration</h3>

<p>Let \(f:[a, b] \longrightarrow\mathbb{R}\) be a bounded function. A
<strong>partition</strong> of \([a,b]\) is a set \(P = \{x_0, x_1, x_2,...,x_n\} :\)
\(a = x_0 &lt; x_1 &lt; x_2 &lt; ... &lt; x_{n-1} &lt; x_n = b.\) For such a partition P
let</p>

<ul>
  <li>\(m_r\) as the greatest lower bound
\(\{f(x): x_{r-1} \leq x \leq x_r\}\)</li>
  <li>\(M_r\) as the least upper bound \(\{f(x): x_{r-1} \leq x \leq x_r\}\)</li>
</ul>

<p>Also let the lower sum \(L(f, P\) and the upper sum \(U(f, P)\)</p>

\[\begin{align} L(f, P)  &amp; = \sum_{r=1}^{n} (x_r - x_{r-1})m_r, \\
U(f, P)  &amp; = \sum_{r=1}^{n} (x_r - x_{r-1})M_r. \end{align}\]

<p>These
are the lower and upper estimates of the area under the graph of \(f\).
For more points in the partition, we get a better estimate of an area.</p>

<p>If there is a unique number A: \(L(f,P) \leq A \leq U(f,P)\) for every
partition P of \([a, b]\) then we say that \(f\) is <strong>integrable</strong> over
\([a, b]\). A is the <strong>definite integral</strong> of \(f\) between \(a, b\).
\(\vartriangleright\)</p>

\[\begin{align}  &amp; \int_{a}^{a} f(x)dx = 0  &amp; \int_{a}^{b} f(x)dx = -
\int_{b}^{a} f(x)dx. \end{align}\]

<p>Some basic integrable functions (referring to the range \([a, b]\)):</p>

<ol>
  <li>Any continuous function.</li>
  <li>Any function which is increasing.</li>
  <li>Any function which is decreasing.</li>
</ol>

<p>After this section are basic rules for integrals. However, I'm going to
put collapsible sections in here for more integration techniques one can
use.</p>

<h3 id="integration-by-substitution">Integration by Substitution</h3>

<p>Like reverse-engineering a chain
rule, this methods relies on the integrand being able to be written in
the following form: \(\int f(g(x))g'(x)dx\) Where the function \(g(x)\) is
contained within a bigger function, and its derivative is outside as a
separate factor.</p>

<p>We replace \(g(x)\) with a new variable, such as \(u\) and \(g'(x)dx\) with
\(du\), and integrate over just the function of \(u\), \(\int f(u) du\),
replacing in what \(u\) is meant to be afterwards.</p>

<p>Why it works...</p>

<p>Let \(u = g(x)\). If we differentiate \(g(x)\) we get
\(\frac{du}{dx} = g'(x)\) Now, look at the integral,
\(\int f(g(x))g'(x)dx\). Replace every \(g\) instance with the equivalent
\(u\) form to get</p>

\[\begin{align} \int f(g(x))g\'(x)dx  &amp; = \int f(u)
\frac{du}{dx} dx \\  &amp; = \int f(u) du. \end{align}\]

<p>Try working out \(\int 2x\cos(x^2) dx\) this way, and
\(\int \frac{9x^2 + 2}{3x^3+2x+7} dx\).</p>

<p>Answers...</p>

<ol>
  <li>
\[-\sin(x^2)\]
  </li>
  <li>
\[\ln(3x^3 + 2x + 7)\]
  </li>
</ol>

<p>Tangentially related is the <strong>reverse chain rule</strong> itself. Say we have
an integral, \(\int \cos(3x^2+2x) dx\), we can note that
\(\sin (3x^2 + 2x)\) differentiates into \((6x+2) \cos(3x^2+2x) dx\) by
chain rule, so integrating the original integrand back we divide by this
extra factor, to get \(\frac{\sin(3x^2+2x)}{6x + 2}\).</p>

<p>In certain cases (i.e. like \(\int \frac{1}{1 + x^2} dx\)) we would have
to substitute trig functions - in this case \(x = \tan \theta\).
(\(dx = \sec^2 (\theta)d\theta\).) Then when we substitute, using said
example we get \(\int \frac{1}{\sec^2 \theta} \sec^2 \theta \: d\theta\).
This is just \(\int 1 d\theta\) and is \(\theta + c\), or \(\tan^{-1} x + c\).</p>

<h3 id="integration-by-parts">Integration by Parts</h3>

<p>Integration by parts is just a formula:</p>

\[\begin{align}  &amp;  \int f(x)g(x)dx = f(x) \int g(x)dx - \int f'(x)
[\int g(x) dx] dx, \\ \textrm{or }  &amp;  \int uv\: dx = u \int v\:
dx - \int u' [\int v \: dx]\: dx. \end{align}\]

<p>It can
alternatively be written \(\int u \: dv = uv + \int v \: du\)</p>

<p>Try work out \(\int x \cos (x )dx\).</p>

<p>Answer...</p>

\[\begin{align} \int x \cos (x )dx  &amp; = x \sin(x) - \int 1 \sin(x) dx
\\  &amp; = x \sin x + \cos x + c. \end{align}\]

<h3 id="properties-of-definite-intervals">Properties of definite intervals</h3>

<ol>
  <li><strong>Sum rule:</strong> \(f, g\) are integrable then so is \(f+g\), and
\(\int_{a}^{b} (f(x) + g(x))dx = \int_{a}^{b} f(x)dx + \int_{a}^{b} g(x)dx.\)</li>
  <li><strong>Multiple (scalar) rule:</strong> \(f\) integrable,
\(\lambda \in \mathbb{R}\), then \(\lambda f\) integrable, and
\(\int_a^b \lambda f(x)dx = \lambda \int_a^b f(x)dx.\)</li>
  <li>\(f\) integrable over \([a,c]\) and \([c,b]\), then \(f\) also integrable
over \([a,b]\), and
\(\int_a^b f(x)dx = \int_a^c f(x)dx + \int_c^b f(x)dx.\)</li>
  <li>If \(f(x) \leq g(x) \; \forall x \in [a, b]\) then provided both
integrals exist, \(\int_a^b f(x)dx \leq \int_a^b g(x)dx.\)</li>
</ol>

<h3 id="fundamental-theorems-of-calculus">Fundamental theorems of calculus</h3>

<h4 id="first-fundamental-theorem-of-calculus">First Fundamental Theorem of Calculus</h4>

<p>Let
\(f : [a,b] \longrightarrow \mathbb{R}\) be integrable, and define
\(F : [a,b] \longrightarrow \mathbb{R}\) as \(F(x) = \int_a^x f(t)dt.\) If
\(f\) continuous at \(c \in (a, b)\) then \(F\) is differentiable at \(c\), and
\(F'(c) = f(c).\)</p>

<h4 id="second-fundamental-theorem-of-calculus">Second Fundamental Theorem of Calculus</h4>

<p>Let
\(f : [a,b] \longrightarrow \mathbb{R}\) be continuous and suppose \(F\) is
a differentiable function : \(F' = f\), then \(\int_a^b f(x)dx = [F(x)]^b_a\)
Where \([F(x)]^b_a\) denotes \(F(b) - F(a)\)</p>

<p>Alternatively, \(\frac{d}{dx}F(x) = f(x) \; \forall x\), and \(f\)
continuous, then \(\int f(x)dx = F(x) + c\) for some c.</p>

<h2 id="logs-and-exponentials">Logs and Exponentials</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note25.pdf">Link to the PDF.</a></p>

<p>In the notes \(\log x\) means the <em>natural</em> log of \(x\), but here I will
use \(\ln x\). It is always useful to specify.</p>

<p>For each \(x &gt; 0\); \(\ln x = \int_1^x \frac{1}{t}dt.\)</p>

<h3 id="properties-of-logarithms">Properties of Logarithms</h3>

<ol>
  <li>
\[\ln(1) = 0.\]
  </li>
  <li>log is a strictly increasing function.</li>
  <li>log is a differentiable function, \(\frac{d(\ln x)}{dx}\)</li>
  <li>
\[\ln(xy) = \ln x + \ln y\]
  </li>
  <li>
\[\ln(\frac{x}{y}) = \ln x - \ln y\]
  </li>
  <li>\(\ln : (0, \infty) \longrightarrow \mathbb{R}\) is bijective</li>
</ol>

<p>Since \(\ln : (0, \infty) \longrightarrow \mathbb{R}\) is bijective, it
has an inverse function \(\exp : \mathbb{R} \longrightarrow (0, \infty)\).
\(e = \exp(1); e^x = \exp(x)\).</p>

<h3 id="properties-of-exponentials">Properties of Exponentials</h3>

<p>For any \(x, y \in \mathbb{R}\),</p>

<ol>
  <li>\(\exp(x+y) = \exp(x)\exp(y)\) but likewise \(e^{x+y} = e^xe^y\)</li>
  <li>\(\frac{d(\exp(x))}{dx} = \exp(x)\), \(\exp(0) = 1\)</li>
  <li>
\[\exp(x) = \lim_{n \rightarrow \infty} (1 + (\frac{x}{n})^n\]
  </li>
  <li>
\[\exp(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!}\]
  </li>
</ol>

<p>For \(a &gt; 0, x \in \mathbb{R}\), any exponential can be defined by
\(a^x = e^{x \ln a}\) Then for \(a, b &gt; 0; x, y \in \mathbb{R}\) the
following properties hold:</p>

\[\begin{align}  &amp; \ln (a^x) = x \ln a
 &amp; (ab)^x = a^xb^x \\  &amp; a^xa^y = a^{x+y}  &amp; (a^x)^y = a^{xy} =
(a^y)^x. \end{align}\]

<p>We can change bases of logarithms (provided \(b \neq 1\));
\(\log_b x = \frac{\ln x}{\ln b}\)</p>

<h2 id="taylors-theorem">Taylor's Theorem</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note26.pdf">Link to the PDF.</a></p>

<h3 id="taylors-theorem-1">Taylor's Theorem</h3>

<p>Let \(f\) be an \(n+1\) times differentiable function
on an open interval containing some points \(a, x\). Then
\(f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + ... + \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x)\)
Where \(R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}\) For some number
\(c\) between \(a, x\).</p>

<p>The resulting polynomial from evaluating some \(n\) terms is called the
<strong>Taylor polynomial of degree \(n\) of \(f\) at \(a\)</strong>. This is however an
<em>approximation</em>, and so the end term \(R_n(x)\) is the error in the
approximation.</p>

<h3 id="taylor-series">Taylor series</h3>

<p>If we can show \(R_n (x) \rightarrow 0\) as \(n \rightarrow \infty\) then we
get a sequence of better and better approximations until we get the
<strong>Taylor series</strong>,
\(f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!}(x-a)^n\) This will only
converge for values of \(x\) within the radius of convergence of the power
series. They can be used to approximate functions.</p>

<p>When \(n = 0\), Taylor's reduces down to the mean value theorem. <em>(Proof
of Taylor's is omitted)</em></p>

<h3 id="nth-derivative-test-for-the-nature-of-stationary-points">Nth derivative test for the nature of stationary points</h3>

<p>Suppose
a function \(f\) has a stationary point at \(a\) however
\(f'(a) = f''(a) = ... = f^{(n-1)} = 0\), while \(f^{n}(a) \neq 0\). If
\(f^{n}(a)\) is continuous,</p>

<ol>
  <li>\(n\) even, \(f^{(n)}(a) &gt; 0\); \(f\) has a local maximum</li>
  <li>\(n\) even, \(f^{(n)}(a) &lt; 0\); \(f\) has a local minimum</li>
  <li>\(n\) odd; \(f\) has a point of inflection at \(a\)</li>
</ol>

<h3 id="maclaurin-series">Maclaurin Series</h3>

<p>When \(a = 0\), we get the Maclaurin's series, which is a special case of
Taylor's series \(f(x) = \sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!}x^n\)</p>

<h4 id="important-maclaurin-series">Important Maclaurin Series</h4>

\[\begin{align} e^x  &amp; = 1 + x + \frac{x^2}{2!} + ... +
\frac{x^n}{n!}  &amp; (x \in \mathbb{R}) \\ \sin x  &amp; = x -
\frac{x^3}{3!} + \frac{x^5}{5!} - ... +
\frac{(-1)^nx^{2n}}{(2n)!}  &amp; (x \in \mathbb{R}) \\ \cos x  &amp; = 1 -
\frac{x^2}{2!} + \frac{x^4}{4!} - ... +
\frac{(-1)^nx^{2n}}{(2n)!}  &amp; (x \in \mathbb{R}) \\ (1+x)^a  &amp; = 1 +
ax + \frac{a(a-1)x^2}{2!} + ... + {a \choose n}x^n  &amp;  (x \in
\mathbb{R}) \\ \ln (1 + x)  &amp; = x - \frac{x^2}{2} + \frac{x^3}{3}- ... + \frac{(-1)^{n+1}x^n}{n}  &amp; (\|x\| &lt; 1) \\ -\ln (1-x)  &amp; = x \frac{x^2}{2} + \frac{x^3}{3} + ... + \frac{x^n}{n}  &amp; (\|x\| &lt;
1) \end{align}\]

<p>These are all expressible as sums using the nth term, though bear in
mind the last two start from \(n=1\).</p>

<h2 id="first-order-odes">First Order ODEs</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note27.pdf">Link to the PDF.</a></p>

<p>ODEs, Ordinary Differential Equations, are equations including
derivatives, such as \(y' = 4x\) or \(y'' + 4y = x\), etc.</p>

<p>The <strong>order</strong> of a DE represents the <em>highest derivative</em> contained
within it. This section looks at 1st orders.</p>

<p>First order equations of the form \(y' = f(x)\) can be easily solved via
integration. This will give us a <em>general solution</em> with the \(+ c\),
representing a <em>family</em> of curves. If we are given constraints, we can
get a <em>particular solution</em>.</p>

<h3 id="separable-equations">Separable Equations</h3>

<p><strong>A [1st order] ODE is called a **separable</strong>
equation if it can be written in the form \(\frac{dy}{dx} = f(x)g(y)\)
Since this can be rewritten as
\(\frac{1}{g(y)} \frac{dy}{dx} = f(x) \textrm{ or } \frac{1}{g(y)}dy = f(x)dx\)
Which can be directly integrated,
\(\int \frac{1}{g(y)} dy = \int f(x)dx.\)</p>

<p>An example of a separable equation is \(\frac{dy}{dx} + xy = 0\).</p>

<h3 id="homogenous-equations">Homogenous Equations</h3>

<p>An ODE is called <strong>homogenous</strong> if it can be
written in the form \(\frac{dy}{dx} = f(\frac{y}{x})\) We let some
variable \(v = \frac{y}{x} : y = xv\), thus
\(\frac{d(xv)}{dx} = f(v) \Longleftrightarrow x\frac{dv}{dx} + v = f(v) \Longleftrightarrow \frac{dv}{dx} = \frac{f(v) -v}{x}\)
This brings it into a separable form, into</p>

\[\begin{align}
\frac{1}{f(v)-v}dv = \frac{1}{x}dx  &amp; \implies \int \frac{1}{x} dx =
\int \frac{1}{f(v)-v} dv \\  &amp; \implies \ln x = \int
\frac{1}{f(v)-v} dv. \end{align}\]

<p>And replace \(v\) by \(\frac{y}{x}\) to
get the original general equation.</p>

<p>The following exercise <em>may</em> help.</p>

<h4 id="exercise-4">Exercise</h4>

<p>Solve \(2xy \frac{dy}{dx} = y^2 - x^2.\)</p>

<p><strong><em>Solution.</em></strong> We can rearrange;
\(\implies \frac{dy}{dx} = \frac{y^2-x^2}{2xy} = \frac{1}{2}(\frac{y^2}{xy} - \frac{x^2}{xy}) = \frac{1}{2}(\frac{y}{x}-\frac{x}{y}).\)
This is homogenous, so we apply the above principle to this equation;</p>

\[\begin{align} \implies \frac{d(vx)}{dx} = \frac{1}{2}(v -
\frac{1}{v})  &amp; \implies x\frac{dv}{dx} + v = \frac{1}{2}(v -
\frac{1}{v}) \\  &amp; \implies \frac{dv}{dx} =
-\frac{1}{x}(\frac{1+v^2}{2v}) \end{align}\]

<p>Which is now in separable
form. Now, skipping straight to \(\ln x = \dots\) form,</p>

\[\begin{align}
\implies \ln x = \int \frac{-2v}{1 + v^2}dv  &amp;  \implies \ln x =
-\ln (1 + v^2) + c \\  &amp; \implies x(1+v^2) = e^c. \end{align}\]

<p>Let
the constant \(k = e^c\) and replacing \(v\) by \(\frac{y}{x}\). Thus
\(x(1 + \frac{y^2}{x^2}) = k \Longleftrightarrow x^2 + y^2 = kx.\)</p>

<h3 id="linear-equations">Linear Equations</h3>

<p>An ODE is called <strong>linear</strong> if it has the form
\(\frac{dy}{dx} + P(x)y = Q(x).\)</p>

<p>Related to this is the <strong>integrating factor</strong>,
\(I(x) = e^{\int P(x) dx}\). This can be derived if \(Q(x) = 0\).</p>

<p>We multiply both sides of the equation by \(I(x)\), to get
\(\frac{dy}{dx}I(x) + yI(x)P(x) = Q(x)I(x)\) Note the LHS is the result of
the implicit differentiation of \(yI(x)\), thus</p>

\[\begin{align}
\frac{d}{dx}(yI(x))  &amp; = Q(x)I(x) \\ yI(x)  &amp; = \int Q(x)I(x) dx.
\end{align}\]

<h4 id="exercise-5">Exercise</h4>

<p>Find the general solution to
\((1+x^2)\frac{dy}{dx} + xy = x\sqrt{1+x^2},\) and the particular solution
which satisfies \(y = \frac{1}{2}, x=0\).</p>

<p><strong><em>Solution.</em></strong> We can rearrange the equation to
\(\frac{dy}{dx} + \frac{xy}{1+x^2} = \frac{x}{\sqrt{1+x^2}}\) The
integrating factor is thus
\(e^{\int \frac{x}{1+x^2}dx} = e^{\frac{1}{2} \ln (1+x^2)} = (1 + x^2)^{\frac{1}{2}}.\)
Multiplying by this integrating factor,</p>

\[\begin{align} \sqrt{1+x^2}
\frac{dy}{dx} + \frac{xy}{\sqrt{1+x^2}}  &amp; = x \\
\frac{d}{dx}(\sqrt{1+x^2} y)  &amp; = x \end{align}\]

<p>Hence the general
solution is \(\sqrt{1+x^2}y = x^2 / 2 + c.\) The particular solution is
when you substitute the values for \(y, x\) into the equation to find c,
and \(c = \frac{1}{2}\). That gives you the final specific curve.</p>

<h2 id="second-order-odes">Second Order ODEs</h2>

<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs131/part4/note28.pdf">Link to the PDF.</a></p>

<p>These are where there is a \(y''\) term, and are dealt with very similarly
to recurrences. They take the form \(ay'' + by' + cy = f(x)\)</p>

<p>If \(f(x) = 0\), then the equation is <strong>homogenous</strong>. To find the general
solution, find the <em>general solution</em> of \(ay'' + by' + cy = 0\) find the
<em>particular solution</em> of \(ay'' + by' + cy = f(x)\) and add them together.</p>

<h3 id="solving-homogenous-equations">Solving homogenous equations</h3>

<p>For the homogenous equation
\(ay'' + by' + cy = 0\)</p>

<p>There's a lot of waffle, but we have an <em>auxiliary equation</em> that is
relevant, which is \(a\lambda^2 + b\lambda + c = 0\)</p>

<p>The roots of this equation determine the form of the general solution.</p>

<ul>
  <li><em>Roots are real and distinct.</em> The general solution is
\(y = Ae^{\lambda_1 x} + Be^{\lambda_2 x}\).</li>
  <li><em>Roots are equal.</em> The general solution is
\(y = Ae^{\lambda x} + Bxe^{\lambda x} = (A + Bx) e^{\lambda x}\).</li>
  <li><em>Complex Roots.</em> Let \(\lambda_1 = \alpha + i\beta\) and
\(\lambda_2 = \alpha - i\beta\). Then, the general equation is
\(y = e^{\alpha x} (A \cos \beta x + B \sin \beta x)\).</li>
</ul>

<h3 id="finding-particular-solutions">Finding particular solutions</h3>

<p>For the equation
\(ay'' + by' + cy = f(x)\) Like in recurrences, we want <em>like</em> functions.</p>

<table>
  <thead>
    <tr>
      <th>Form of \(f(x)\)</th>
      <th>Form for a particular solution</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\(e^{\alpha x}\)</td>
      <td>-   \(y = Ae^{\alpha x}\) if \(\alpha\) is not a root of the auxiliary equation<br /> -   \(y = Axe^{\alpha x}\) if \(\alpha\) is one root<br /> -   \(y = Ax^2 e^{\alpha x}\) if \(\alpha\) is the repeated roots</td>
    </tr>
    <tr>
      <td>Polynomial of degree \(n\)</td>
      <td>-   Polynomial of degree \(n\) if 0 is not a root<br />-   Polynomial of degree \(n+1\) if 0 is a non-repeated root<br />-   Polynomial of degree \(n+2\) if 0 is a repeated root</td>
    </tr>
    <tr>
      <td>\(A \cos \alpha x + B \sin \alpha x\)</td>
      <td>-   \(y = C \cos \alpha x + D \sin\alpha x\) if \(\alpha i\) is not a root of the auxiliary equation<br />-   \(y = x(C \cos \alpha x + D \sin \alpha x)\) otherwise</td>
    </tr>
  </tbody>
</table>


            <br/>
            <hr style="
                padding: 5px;
                border-radius: 1em;
                background-color: whitesmoke;
            ">
            <br/>


                <footer class="site-footer">

                    
                    <span class="site-footer-owner"><a href="https://github.com/CSRG-Group/dcs-notes.github.io">dcs-notes.github.io</a> is maintained by <a href="https://github.com/CSRG-Group">CSRG-Group</a>.</span>
                    
                </footer>
            </main>
        </div>
    </div>
</body>

</html>
</html>